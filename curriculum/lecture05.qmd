# Lecture 5: Logistic regression

* Remember the note on the log loss.
* Formula for the probability and an exercise on deriving it. (This is one of the key take-aways.)
* **Bayes classifier:** Assigns an observation to the class where $p_k(x)$ is highest.
* Logistic regression and LDA fit the same function family, but in different ways.

* "Thus, we expect LDA to outperform logistic regression when the normality assumption (approximately) holds, and we expect logistic regression to perform better when it does not." (p. 163) 
  * Sounds strange - how about the conditions for logistic regression to work? Shouldn't they matter?
  
* Is QDA = LDA with all polyomial features? No, since they do not use the same fitting methods. Obviously the LDA will not be normal, but it may still tend to outperform QDA. 

### Short exercises
* Exercises with ROC curve.

### 