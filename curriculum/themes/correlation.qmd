---
title: "Correlation matrices and pair plots"
author: Jonas Moss
date: last-modified
execute:
  echo: true
jupyter: python3
format:
  html: 
    theme: lumen
    fontsize: 1.1em
    linestretch: 1.7
    mainfont: "Verdana"
reference-location: margin
citation-location: margin
fig-cap-location: top
engine: jupyter
toc: true
lightbox: true
---

## Takeaways

1. Know the basic properties of correlations and how to interpret them.
2. Non-zero correlation does not imply causality, neither does causality imply non-zero correlation.
3. Construct color-coded correlation matrices with Pandas. Be sure to choose the right precision! Use the snipped `xcorr` for this.
4. Use `pairplot` to spot non-linearities, problems with the data, and identify potential multicollinearity problems.
5. Use Spearman's correlation to measure monotone associations.

## Correlation

```{python}
# | echo: False
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.formula.api as smf
```

1. Definition of correlation.
2. Measures linearity; Anscombe's quartet.

## Correlation and causality

![Correlation is not causation. Source: [xkcd 552](https://xkcd.com/552/)](correlation.png)

## Correlation

The correlation coefficient 

### Population definition
$$\textrm{Cor}(X,Y)=\frac{\textrm{Cov}(X,Y)}{\sqrt{\textrm{Var}(X)}\sqrt{\textrm{Var}(Y)}}$$

### Sample definition

### Properties

1. $-1 \leq \textrm{Cor}(X,Y)\leq 1$.
2. $\textrm{Cor}(X,Y) = 1$ if and only if $Y=a+bX$ for some $b>0$.
3. $\textrm{Cor}(X,Y) = -1$ if and only if $Y=a+bX$ for some $b<0$.
4. $\textrm{Cor}(X,Y) = 0$ when $X$ and $Y$ are independent.

### When is a correlation large?


```{python}
rng = np.random.default_rng(seed=313)
gen = lambda rho: rng.multivariate_normal([1, 1], [[1, rho], [rho, 1]], 100)
rhos = [-1, -0.8, -0.5, -0.2, 0, 0.2, 0.5, 0.8, 1]
arr = np.vstack([gen(rho) for rho in [-1, -0.8, -0.5, -0.2, 0, 0.2, 0.5, 0.8, 1]])
df = pd.DataFrame(
    {
        "rho": np.hstack([np.repeat(rho, 100) for rho in rhos]),
        "x": arr[:, 0],
        "y": arr[:, 1],
    }
)
g = sns.lmplot(df, x="x", y="y", col = "rho",col_wrap=3, fit_reg=False)
g.set_axis_labels("", "")
```

It's hard to distinguish the plots with correlations $-0.2$, $0$ and $0.2$.

![Always look at the data and don't be impressed by small correlations ($0.25$ here). Source: [xkcd 552](https://xkcd.com/552/)](linear_regression.png)

Improve your intuition by playing [Guess the Correlation](https://www.guessthecorrelation.com). ![Example screen from Guess the Correlation](guess_the_correlation.png)

So, what's a large correlation, medium correlation, small correlation? The boring answer is that it depends both on the field and what we want to use the correlation for.

(Five examples here)

| Name  | Field      | Small | Medium | Large |
| ----  | ---------- | ----- | ------ | ----- |
| Cohen | Psychology | $0.1  | $0.3$  | $0.5$  |
| Quintana | Heart rate variability | $0.26$ | $0.51$ | $0.88$ |


In exploratory data analysis we often want to use the correlations as input into other models. Then we care mostly about removing correlations with very small (less than $0.1$). Perhaps surprisingly, we may also have to remove observations with very high correlations (e.g., higher than $0.95$), since they contain the same information about. Highly correlated features are called "multicollinear", and cause the most common models such as logistic regression models and linear regression models to become unstable, and might 




```{python}


```

### Correlation matrices
https://ggplot2.tidyverse.org/reference/diamonds.html
```{python}
diamonds = sns.load_dataset("diamonds")
diamonds.head()
```

```{python}
# | error: True
diamonds.corr()
```

```{python}
diamonds.corr(numeric_only=True)
```

```{python}
diamonds.corr(numeric_only=True).style.background_gradient(cmap="coolwarm", axis=None)
```

```{python}
diamonds.corr(numeric_only=True).style.background_gradient(
    cmap="coolwarm", axis=None, vmin=-1, vmax=1
).format(precision=2)

```

### Anscombe's quartet

```{python}
# | echo: False
# | eval: False
anscombe = sns.load_dataset("anscombe")
zz = anscombe.groupby("dataset").corr()
zz[["x", "y"]].iloc[0::2, -1]

```

```{python}
# | echo: False
# | fig-cap: Ansbombe's quartet. All data sets have the same sample correlation, $0.816$. Only the plot in the upper-left is linear with noise.

sns.set_theme(style="ticks")
# Load the example dataset for Anscombe's quartet
df = sns.load_dataset("anscombe")
# Show the results of a linear regression within each dataset
sns.lmplot(
    data=df,
    x="x",
    y="y",
    col="dataset",
    hue="dataset",
    col_wrap=2,
    palette="muted",
    ci=None,
    height=4,
    scatter_kws={"s": 50, "alpha": 1},
)
```

* **Figure 1.** Looks like $y_i = a + bx_i + \epsilon_i$, so we have linearity with noise. This is what we expect.
* **Figure 2.** Here it appears that $y = a+bx+bx^2$, with no error. But the correlation can't capture that, since it only measures deviations from linearity.
* **Figure 3.** There seems to be a perfectly linear relationship, but it is ruined by one observation. There is a possibility of data leakage here.
* **Figure 4.** Two categories, but only one observation in one of them. Are we dealing with an outlier again?

### The pairplot function


```{python}
sns.pairplot(diamonds)

```

There are some outliers in 

```{python}
diamonds = sns.load_dataset("diamonds")
diamonds.drop(index=diamonds["z"].nlargest(1).index,inplace = True)
diamonds.drop(index=diamonds["y"].nlargest(5).index,inplace = True)
diamonds.drop(index=diamonds[diamonds["z"] == 0].index,inplace = True)
diamonds.drop(index=diamonds[diamonds["y"] == 0].index,inplace = True)
sns.pairplot(diamonds[["x", "y", "z"]])
```



```{python}
plt.plot(diamonds["y"],smf.ols("carat ~ x*y*z", diamonds).fit().predict(), 'o')

plt.plot(diamonds["y"], diamonds["carat"], 'o')
```

## Spearman's rho

::: {.callout-note icon=false appearance="simple"} 
## Definition
Spearman's rho
:::

Another option is `"kendall"` for Kendall's tau. This is rarely used, however.


```{python}
diamonds.corr(method="spearman", numeric_only=True).style.background_gradient(
    cmap="coolwarm", axis=None, vmin=-1, vmax=1
).format(precision=3)
```

```{python}
rng = np.random.default_rng(seed=1)
x = rng.normal(size=100)
y = 1/2*x**5 + x**3 + 1
sns.scatterplot(None, x=x, y=y)
np.corrcoef(x, y)
#pd.DataFrame({'x':x, 'y':y}).corr(method="spearman")
```