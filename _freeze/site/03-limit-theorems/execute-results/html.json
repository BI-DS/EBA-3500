{
  "hash": "35c1acfdd90d3df953b8b8d2fb6321b2",
  "result": {
    "markdown": "# Limit theorems\n\n::: {.callout-note}\n**Updated 5/9:** Added short note about law of large numbers.\n:::\n\n## Curriculum\n1.  Dekking et al., Chapter 13: The law of large numbers\n\n2.  Dekking et al., Chapter 14: The central limit theorem\n\n3. The short notes below.\n\n## Notes\n\n### Law of large numbers\nThe law of large numbers states that the sample means $\\overline{X}_i=\\frac{1}{n}\\sum_{i=1}^n X_i$ converges to the population mean \n$EX$ in probability as $n\\to \\infty$. We illustrate this result using the \ncontinuous uniform distribution on $[0,1]$, called `rng.uniform` in Numpy.\nThe density of this distribution is $f(x)=1[x\\in[0,1]]$, where $1(x\\in A)$ denotes the indicator function you are used to from the previous probability course. (It is commonly written as $I[x\\inA]$ or just $I(A)$.) Its expectation is equal to \n$1/2$.\n\n#### Quick exercise\nShow that the expected value of the uniform distribution on $[a,b]$ is $(b+a)/2$ using the definition $f(x;a,b) = 1/(b-a)1[x\\in[a,b]]$. \n::: {.callout-tip collapse=\"true\"}\n#### Solution\n$$\\int_a^b x/(b-a)dx = \\frac{1}{2}(b^2-a^2)/(b-a) = \\frac{1}{2}(b+a)(b-a)/(b-a) = \\frac{1}{2}(b+a)$$.\n:::\n\nWe first simulate $n=10000$ of uniforms and store them `x`. These correspond to a sequence $X_1,X_2,X_3,\\ldots X_n$ of independently and identically distributed\nuniform variables. \n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pylab as plt\n\nrng = np.random.default_rng(seed=313)\nx = rng.uniform(0, 1, 10000)\n```\n:::\n\n\nNow we compute the partial means $\\overline{X}_j = \\frac{1}{n}\\sum_{i=1}^n X_i$. This is most conveniently done using the `np.cumsum` function, which calculate the cumulative sums of `x`, i.e., `y[i] = x[0:(i+1)].sum()` when `y=np.cumsum(x)`.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nmeans = np.cumsum(x) / (np.arange(len(x)) + 1)\n```\n:::\n\n\nNow we can illustrate the law of large numbers:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nplt.plot(means)\nplt.axhline(y = 0.5)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](03-limit-theorems_files/figure-html/cell-4-output-1.png){width=588 height=411}\n:::\n:::\n\n\nObserve that the partial means get arbitrarily close to the true expected value ($0.5$) as $n$ increases. \n\n#### Quick exercise\nMake a function `sim_lln(n, gen)` that takes a number `n` and a random number generator such as `lambda x: rng.uniform(2, 3, x)`, simulates `n` numbers, and makes a plot similar to the one above.\n\n### Central limit theorem\nInformally speaking, the central limit theorem states that whenever $\\{X_i\\}_{i=0}^n$ are independently and identically distributed according to some distribution $F$ with mean $\\mu$ and standard deviation $\\sigma$ (in mathematical notation $X_i \\stackrel{iid}{\\sim} F$) then $\\sqrt{n}\\frac{\\overline{X} - \\mu}{\\sigma}$ is approximately normally distributed. (Here $\\overline{X}$ denotes the sample mean.) There is a more precise statements of this result, but that is beyond the scope of this course; see e.g. wikipedia for more details.\n\nIt is fairly easy to illustrate the central limit theorem (CLT) using simulations.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nimport numpy as np\nrng = np.random.default_rng(seed=313)\n\nn = 100\nx = rng.gamma(3, 4, (n, 10000))\nmeans = x.mean(axis = 0)\nrescaled = np.sqrt(n) * (means - x.mean()) / np.std(x, ddof = 1)\n```\n:::\n\n\nHere the `rescaled`, the rescaled sample means, approximates the distribution of $\\sqrt{n}\\frac{\\overline{X} - \\mu}{\\sigma}$.\n\nLet's simulate some true normal values to verify this:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nnormals = rng.standard_normal(10000)\n```\n:::\n\n\nNow we can plot.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nimport seaborn as sns\nimport matplotlib.pylab as plt\nplt.clf()\nsns.histplot(rescaled, stat = \"density\")\nsns.histplot(normals, stat = \"density\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](03-limit-theorems_files/figure-html/cell-7-output-1.png){width=597 height=411}\n:::\n:::\n\n\nObserve that the histograms overlap closely. \n\n**Quick exericise:** If you run the simulation with $n=10$ instead of $n=100$, do\nyou think the histograms will match more or less closely? Run the simulation to\nverify. \n\n**Quick exercise:** Change the parameters of the gamma distribution `rng.gamma`;\nlook up the documentation if you have to. Can you find parameters where the\nnormal approximation in the central limit theorem is really poor when $n=100$?\n\n## Exercises\n### Chapter 13\n* 13.1: Use Python to simulate from these variables. You can find all of these\ndistributions in `np.random`, just initialize an `rng` object first! If you\nstruggle finding the distributions, take a look at the Numpy documentation. Use\nthe `.mean` method to approximate the probabilities and Wikipedia for the standard\ndeviations (or use Numpy here as well.)\n* 13.2\n* 13.3\n* 13.5\n* 13.7: The function $F_n$ is known a the empirical cumulative distribution \nfunction, shortened to `ecdf`. You can use Numpy to verify the claims of this\nexercise; write your own function or use the `ecdf` function from the\nstatsmodels packae, `from statsmodels.distributions.empirical_distribution import ECDF`.\n* 13.9: If you are not able to find the required $a$, find it using simulations.\n* 13.12: (Optional)\n\n### Chapter 14\nIn some of these exercises you will need the quantile function for the normal\ndistribution. You can find this in the `scipy` package, where it is called the\n`ppf`.\n\nHere is an example of $\\Phi^{-1}(0.95)$.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nfrom scipy.stats import norm \nnorm.ppf(0.95, loc=0, scale=1)\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\n1.6448536269514722\n```\n:::\n:::\n\n\n* 14.1: Solve this using Python.\n* 14.2: You can use `np.random.beta` to simulate from the Beta distribution. The\nparameters corresponding to the distribution in the exercise is $a=1$, $b=4$.\nUse this to compare the normal approximation to the true distribution of means.\n(*Hint:* Simulate a $625$ variables a bunch of times and use the mean function. \nYou can either calculate the variance of $X$ yourself or use wikipedia.)\n* 14.8\n* 14.3\n* 14.4\n* 14.7\n* 14.9: This is an exercise about translating a textual problem into a mathematical one! It's always a good idea to do all translations first and calculations afterward.\n\n",
    "supporting": [
      "03-limit-theorems_files\\figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}