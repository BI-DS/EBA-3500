{
  "hash": "9d509f869e46577179144070349f3d6f",
  "result": {
    "markdown": "# Limit theorems\n\n::: {.callout-note}\n**Updated 12/9:** Added note to exercises and removed a redundant comment.\n:::\n\n## Curriculum\n1.  Dekking et al., Chapter 13: The law of large numbers\n\n2.  Dekking et al., Chapter 14: The central limit theorem\n\n3. The short notes below.\n\n## Notes\n\n### Law of large numbers\nThe law of large numbers states that the sample means $\\overline{X}_i=\\frac{1}{n}\\sum_{i=1}^n X_i$ converges to the population mean \n$EX$ in probability as $n\\to \\infty$. We illustrate this result using the \ncontinuous uniform distribution on $[0,1]$, called `rng.uniform` in Numpy.\nThe density of this distribution is $f(x)=1[x\\in[0,1]]$, where $1(x\\in A)$ denotes the indicator function you are used to from the previous probability course. (It is commonly written as $I[x\\inA]$ or just $I(A)$.) Its expectation is equal to \n$1/2$.\n\n#### Quick exercise\nShow that the expected value of the uniform distribution on $[a,b]$ is $(b+a)/2$ using the definition $f(x;a,b) = 1/(b-a)1[x\\in[a,b]]$. \n\n::: {.callout-tip collapse=\"true\"}\n#### Solution\n$$\\int_a^b x/(b-a)dx = \\frac{1}{2}(b^2-a^2)/(b-a) = \\frac{1}{2}(b+a)(b-a)/(b-a) = \\frac{1}{2}(b+a)$$.\n:::\n\nWe first simulate $n=10000$ of uniforms and store them `x`. These correspond to a sequence $X_1,X_2,X_3,\\ldots X_n$ of independently and identically distributed\nuniform variables. \n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pylab as plt\n\nrng = np.random.default_rng(seed=313)\nx = rng.uniform(0, 1, 10000)\n```\n:::\n\n\nNow we compute the partial means $\\overline{X}_j = \\frac{1}{n}\\sum_{i=1}^n X_i$. This is most conveniently done using the `np.cumsum` function, which calculate the cumulative sums of `x`, i.e., `y[i] = x[0:(i+1)].sum()` when `y=np.cumsum(x)`.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nmeans = np.cumsum(x) / (np.arange(len(x)) + 1)\n```\n:::\n\n\nNow we can illustrate the law of large numbers:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nplt.plot(means)\nplt.axhline(y = 0.5)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](03-limit-theorems_files/figure-html/cell-4-output-1.png){width=588 height=411}\n:::\n:::\n\n\nObserve that the partial means get arbitrarily close to the true expected value ($0.5$) as $n$ increases. \n\n#### Quick exercise\nMake a function `sim_lln(n, gen)` that takes a number `n` and a random number generator such as `lambda x: rng.uniform(2, 3, x)`, simulates `n` numbers, and makes a plot similar to the one above.\n\n### Central limit theorem\nInformally speaking, the central limit theorem states that whenever $\\{X_i\\}_{i=0}^n$ are independently and identically distributed according to some distribution $F$ with mean $\\mu$ and standard deviation $\\sigma$ (in mathematical notation $X_i \\stackrel{iid}{\\sim} F$) then $\\sqrt{n}\\frac{\\overline{X} - \\mu}{\\sigma}$ is approximately normally distributed. (Here $\\overline{X}$ denotes the sample mean.) There is a more precise statements of this result, but that is beyond the scope of this course; see e.g. wikipedia for more details.\n\nIt is fairly easy to illustrate the central limit theorem (CLT) using simulations.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nimport numpy as np\nrng = np.random.default_rng(seed=313)\n\nn = 100\nx = rng.gamma(3, 4, (n, 10000))\nmeans = x.mean(axis = 0)\nrescaled = np.sqrt(n) * (means - x.mean()) / np.std(x, ddof = 1)\n```\n:::\n\n\nHere the `rescaled`, the rescaled sample means, approximates the distribution of $\\sqrt{n}\\frac{\\overline{X} - \\mu}{\\sigma}$.\n\nLet's simulate some true normal values to verify this:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nnormals = rng.standard_normal(10000)\n```\n:::\n\n\nNow we can plot.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nimport seaborn as sns\nimport matplotlib.pylab as plt\nplt.clf()\nsns.histplot(rescaled, stat = \"density\")\nsns.histplot(normals, stat = \"density\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](03-limit-theorems_files/figure-html/cell-7-output-1.png){width=597 height=411}\n:::\n:::\n\n\nObserve that the histograms overlap closely. \n\n**Quick exericise:** If you run the simulation with $n=10$ instead of $n=100$, do\nyou think the histograms will match more or less closely? Run the simulation to\nverify. \n\n**Quick exercise:** Change the parameters of the gamma distribution `rng.gamma`;\nlook up the documentation if you have to. Can you find parameters where the\nnormal approximation in the central limit theorem is really poor when $n=100$?\n\n## Exercises\n### Chapter 13\nShort solution proposals can be found in the book.\n* 13.1:(**This exercise requires more work than the rest.)** Use Python to simulate from these variables. You can find all of these\ndistributions in `np.random`, just initialize an `rng` object first! If you\nstruggle finding the distributions, take a look at the Numpy documentation. Use\nthe `.mean` method to approximate the probabilities and Wikipedia for the standard\ndeviations (or use Numpy here as well.) *Note:* Be sure to do the exercise for\none $k$ and one distribution first, then generalize! \n* 13.2\n* 13.3\n* 13.5\n* 13.7: The function $F_n$ is known a the empirical cumulative distribution \nfunction, shortened to `ecdf`. You can use Numpy to verify the claims of this\nexercise; write your own function or use the `ecdf` function from the\nstatsmodels package, `from statsmodels.distributions.empirical_distribution import ECDF`.\n* 13.9: If you are not able to find the required $a$, find it using simulations.\n* 13.12: (Optional)\n\n::: {.callout-tip collapse=\"true\"}\n## Solution to some exercises\n\n### Exercise 13.1\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n# The “µ ± a few σ” rule. Most of the probability mass of a\n# random variable is within a few standard deviations from its expectation.\n\n# U(−1, 1), U(−a, a), N(0, 1), N(µ, σ2), Par (3), Geo(1/2).\n\nrng = np.random.default_rng(seed=313)\nn = 100000\n\nrandoms = [\n  rng.uniform(-1, 1, n),\n  rng.uniform(-2, np.pi, n),\n  rng.normal(0, 1, n),\n  rng.normal(1, 2, n),\n  rng.pareto(3, n),\n  rng.geometric(0.5, n)]\n  \nmeans = [np.mean(x) for x in randoms]\nstdevs = [np.std(x) for x in randoms]\n\ndef prob(k, x, mean, std):\n  \"\"\" Approximate P(|X-mean|<k*std).\"\"\"\n  return (np.abs(x - mean) < k * std).mean() \n\nk = 1\n[prob(k, x, mean, std) for x, mean, std in zip(randoms, means, stdevs)]\n\ndef probs(k):\n  \"\"\" List of approximate probabilities.\"\"\"\n  return [prob(k, x, mean, std) for x, mean, std in zip(randoms, means, stdevs)]\n\nnp.array([probs(k) for k in [1, 2, 3, 4]])\n\n# Chebyshev's inequality: P(|Y-\\mu|<k*std) >= 1-1/k^2\n\n[round(1-1/k**2, 3) for k in [1, 2, 3, 4]]\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\n[0.0, 0.75, 0.889, 0.938]\n```\n:::\n:::\n\n\n### Exercise 13.2\nEither integrate or use wikipedia. The mean is $(b+a) / 2$.\n:::\n\n### Chapter 14\nThe cumulative distribution function of a normal variable can be found in Scipy. For instance, when $X$ is normal with mean $0$ and standard deviation $1$. Then we have:\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nfrom scipy.stats import norm \nnorm.cdf(0, loc=0, scale=1)\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\n0.5\n```\n:::\n:::\n\n\nShort solution proposals can be found in the book.\n\n* 14.1: Solve this using Python. You can also simulate variables with the required\nmeans and variances using `np.uniform` with the correct choices of `low` and `high`.\nUse the formulas $E(X)=(b+a) / 2$ and $\\textrm{Var}X = (b-a)^2/12$ and solve\nfor $a$ and $b$.\n* 14.2: You can use `np.random.beta` to simulate from the Beta distribution. The\nparameters corresponding to the distribution in the exercise is $a=1$, $b=4$.\nUse this to compare the normal approximation to the true distribution of means.\n(*Hint:* Simulate a $625$ variables a bunch of times and use the mean function. \nYou can either calculate the variance of $X$ yourself or use wikipedia.)\n* 14.8\n* 14.3\n* 14.4\n* 14.7\n* 14.9: This is an exercise about translating a textual problem into a mathematical one! It's always a good idea to do all translations first and calculations afterward.\n\n::: {.callout-tip collapse=\"true\"}\n### Solution to exercise 14.1 \n\n# CLT: Use that the sum has mean approximate 144 * 2 and variance approximately\n# 144 * 4 (so standard deviation sqrt(144) * 2)\nfrom scipy.stats import norm\nnorm.pdf(144, 144 * 2, np.sqrt(144) * 2)\n\n# The uniform distribution on [a,b] has variance\n# (b-a)^2/12 = 4 and mean (b+a)/2 = 2. We can solve the first to get\n# b = -4a. Then the second equation yields 5^2 a^2 / 12 = 4, so that\n# 12 * 4 / 25 = a^2, with b = -4a. It follows that a = - (2 * sqrt(12)) / 5\n\nimport numpy as np\nrng = np.random.default_rng(seed=313)\na = -2 * np.sqrt(12) / 5\nb = -4 * a\n\n# Verify\nx = rng.uniform(a, b, 100000)\nx.var()\nx.mean()\n\n# Simulate\ny = rng.uniform(a, b, (144, 100000))\nsums = y.sum(axis=0)\n(sums > 144).mean()\n:::\n\n",
    "supporting": [
      "03-limit-theorems_files"
    ],
    "filters": [],
    "includes": {}
  }
}