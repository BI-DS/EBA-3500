A commonly used measure of predictive ability for binary logistic models is
the fraction of correctly classified responses. Here one chooses a cutoff on the
predicted probability of a positive response and then predicts that a response
will be positive if the predicted probability exceeds this cutoff. There are a
number of reasons why this measure should be avoided.


1. It’s highly dependent on the cutpoint chosen for a “positive” prediction.

2. You can add a highly significant variable to the model and have the percentage classified correctly actually decrease. Classification error is a very
insensitive and statistically inefficient measure264, 633 since if the threshold
for “positive” is, say 0.75, a prediction of 0.99 rates the same as one of
0.751.

3. It gets away from the purpose of fitting a logistic model. A logistic model
is a model for the probability of an event, not a model for the occurrence
of the event. For example, suppose that the event we are predicting is
the probability of being struck by lightning. Without having any data,
we would predict that you won’t get struck by lightning. However, you
might develop an interesting model that discovers real risk factors that
yield probabilities of being struck that range from 0.000000001 to 0.001.

4. If you make a classification rule from a probability model, you are being
presumptuous. Suppose that a model is developed to assist physicians
in diagnosing a disease. Physicians sometimes profess to desiring a binary
decision model, but if given a probability they will rightfully apply different
thresholds for treating different patients or for ordering other diagnostic
tests. Even though the age of the patient may be a strong predictor of
the probability of disease, the physician will often use a lower threshold
of disease likelihood for treating a young patient. This usage is above and
beyond how age affects the likelihood.

5. If a disease were present in only 0.02 of the population, one could be 0.98
accurate in diagnosing the disease by ruling that everyone is disease–free,
i.e., by avoiding predictors. The proportion classified correctly fails to take
the difficulty of the task into account.

6. van Houwelingen and le Cessie633 demonstrated a peculiar property that
occurs when you try to obtain an honest estimate of classification error
using cross-validation. The cross-validated error rate corrects the apparent
error rate only if the predicted probability is exactly 1/2 or is 1/2±1/(2n).
The cross-validation estimate of optimism is “zero for n even and negligibly
small for n odd.” Better measures of error rate such as the Brier score and
logarithmic scoring rule do not have this problem. They also have the
nice property of being maximized when the predicted probabilities are the
population probabilities.416