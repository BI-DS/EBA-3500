# Course description {.unnumbered}

-   Learn about basic machine learning methods and concepts.

-   Use `sklearn` to fit basic machine learning models.

-   Gain enough footing to work with basic machine learning on your own.

-   Focused on applications and concepts, not on mathematics!

-   Two exams: One conceptual exam (school; using only questions from the flash cards, 1 hour) and one applied exam (long, home).

-   Curriculum:

    -   Introduction to statistical learning
        -   This is our source for concepts and exercises. We do not use the
        -   Except:
            -   PLS
            -   GAM
            -   Local regression
            -   Smoothing splines
            -   ANOVA etc part in regression; anything about p-values and AIC.
            -   The "labs"
            -   Everything is detailed in the plan.
            -   All of this is worth reading, but is either not important enough for machine learning or not covered by sklearn.
    -   (Hopefully a source on sklearn.)
    -   Documentation of sklearn, selected parts.
    -   The lectures.
    -   Some extra notes (finish before!) \[on log loss, I think is needed; also why MSE might be better\]
    -   All exercises and their solution proposals.
        -   Use most ISPL exercises;
        -   Add some conceptual and programming exercises where needed, but make the programming exercises easy.

-   **Aid:**

    -   Fully solved exercises with explanations.
    -   Flash cards for spaced repetition testing. *Only questions from the flash cards will be on the school exam.*
    -   Conceptual "key take-aways" every week.
    -   List of important functions every week along with their most important arguments.
    -   Interleaved exercises for better learning.
    -   Two mock exams for both home and school exams.
    -   Links for how to dig deeper.
    -   Recommended YouTube explainers.

-   **Regression:** Linear regression, knn, LASSO, ridge, SVR, decision trees.

-   **Classification:** LDA, QDA, knn, naive Bayes, logistic regression, SVM, decision trees.

-   Using PCA and cross-validation with logistic regression: https://scikit-learn.org/stable/auto_examples/compose/plot_digits_pipe.html#sphx-glr-auto-examples-compose-plot-digits-pipe-py

## Plan

| Number | Name | Chapters |
| --     | ---  | ------   |
| 1  | Introduction and recap               | ISLP 1
| 2  | Statistical learning                 | ISLP 2
| 3  | Exploration and exposition           | Notes on exploratory data analysis 
| 4  | More exploration and exposition      | Notes on exploratory data analysis 
| 5  | Linear regression                    | ISPL 3
| 6  | Feature transformations              | ISPL 3
| 7  | Logistic regression                  | ISPL 4
| 8  | Generative models for classification | ISPL 4
| 9  | Cross-validation                     | ISPL 5
| 10 | Regularization                       | ISPL 6
| 11 | Non-linear features                  | ISPL 7
| 12 | Decision trees                       | ISPL 8
| 13 | Boosting and random forests          | ISPL 9
| 14 | Evaluating models                    | Notes on evaluating models
| 15 | Connecting the dots                  | 


## Questions

Make \~300 questions, \~20 for each week? (10 in lecture, 10 after exercises.)

### Flash cards

### Lecture 1: Probability recap

-   What is a cumulative distribution function?

-   What is a probability density function?

-   What is a probability mass function?

-   What is a random variable?

-   Conditional expectation.

-   The variance of a random variable.

-   Standard deviation of a random variable.

-   The sample standard deviation.

-   The unbiased sample variance.

-   Central limit theorem.

-   The sample median.

-   The correlation coefficient.

-   The law of large numbers.

-   Bayes' rule.

-   Monte Carlo study.

-   A non-negative function that integrates to $1$.

-   A non-negative function that starts at $0$ and ends at $1$.

-   The two-sigma rule.

-   Mean squared error.

-   Unbiased estimator.

### Lecture 1: Markdown and Quarto basics

### Lecture 2: Programming and statistics

-   How do you initialize an `rng` object in Numpy?

    -   \`np.random.default_rng(seed=313)

-   Name of `plt`.

    -   `import matplotlib.pylab as plt`

-   Indexing a pandas data frame by column number.

    -   `frame.iloc(i, axis = 1)`

-   Unbiased sample variance in Numpy.

    -   `np.var(x, ddof=1)` or `x.var(ddof=1)`.

-   Scatter plot.

    -   Definition.

-   Function for making scatterplot of all variables.

    -   `sns.pairplot()`.

-   Numpy method to calculate mean for all columns of a Numpy array.

    -   `np.mean(axis=1)`

-   Pandas method for showing correlation matrix.

    -   `frame.corr()`

-   Pandas method for showing correlation matrix with fixed y variable.

-   Dictionary comprehension.

-   Pandas dictionary to data frame.

-   Data frame column to index.

-   Reading data frame as csv.

-   Dictionary values to Numpy array.

-   Ci

### Lecture 3: Statistical learning

-   Cluster analysis
-   Regression
-   Classification
-   Test error and validation error
-   Bayes error
-   Bayes decision boundary
-   Bias-variance trade-off

### Linear regression

-   Residual variance in linear regression.
-   Interpreting regression coefficients.
-   Definition of least squares: The defining equation.
-   The estimating equation.
-   RSS: What does it mean?
-   Predictions from the model
-   Predicting using `sklearn`.
-   Plotting predictions using sort and plot.
-   Fitting regression models using `sklearn`.
-   One hot encoding.
-   Extracting coefficients from a model.
-   $R^2$ and MSE.
-   What could you use these for? The adjusted R\^2, Mallow's C_p, AIC and BIC.
-   Negative adjusted $R^2$.
-   Running regression without intercept (fit_intercept=False).
-   How many linear regression models are there? (2\^p)
-   Understanding predictions (using exam example?)
-   Forward selection
-   Backward selection
-   Mixed selection
-   `feature_selection.SequentialFeatureSelector`
-   Transforming the covariates.
-   `FunctionTransformer`
-   Utility of logtransforms.
-   Interaction terms.
-   `PolynomialFeatures` with or without interaction only.

### Lecture X1 and X2:

-   Bayes classifier
-   LDA
-   QDA
-   Naive Bayes
-   Relationship between LDA, QDA, and Gaussian naive Bayes.
-   Logistic regression
-   Multinomial logistic regression
-   k nearest neighboor
-   *Heuristic.* LDA and logistic work best with linear boundaries
-   *Heuristic.* QDA and Gaussian work best with moderately non-linear boundaries.
-   *Heuristic.*
-   Transforming the response in sklearn
-   Why would you log-transform the response? \[maybe an exercise here too.\]

### Lecture X:

-   Polynomial features.
    -   preprocessing.PolynomialFeatures
-   Log transform
    -   preprocessing.FunctionTransformer(np.log) or
    -   preprocessing.FunctionTransformer(np.log1p)

### Lecture X: Resampling

-   Validation set approach.

-   Formula for the bootstrap standard error.

-   What is bootstrapping?

-   What is K-fold cross-validation?

-   What is leave-one-out cross-validation?

-   What is the approximate probability the $i$th observation is not a bootstrap sample when $n$ is large?

    -   $\exp(-1)$.

-   The `sklearn` function that cross-validates a score.

    -   `model_selection.cross_...`

### 

### Questions based on flash cards

What is the difference between QDA and LDA? 1. QDA means "quadratic discriminant analysis"; this is because it transforms every feature to a quadratic polynomial before fitting. 2. LDA use a fixed covariance matrix, but QDA allows for different covariance matrices. 3. QDA uses a quadratic covariance matrix while LDA uses a linear one. 4. LDA is based on a normal model for the responses, but QDA is based on a quadratic normal model.

What is the link between QDA an Gaussian naive Bayes? 1. 2. 3. 4.

What is the connection between support vector machines and penalized logistic regression? 1.

What is the kernel trick in support vector machines?

What is a cumulative distribution function?

## sklearn functions

#### Model selection

-   [model_selection.cross_val_score](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score)
-   [model_selection.GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV)

#### Metrics

-   [metrics.confusion_matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)
-   [metric.r2_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score)
-   [metrics.mean_absolute_error](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html#sklearn.metrics.mean_absolute_error)
-   [metrics.mean_squared_error](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error)
-   [metrics.log_loss](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html#sklearn.metrics.log_loss)
-   [metrics.auc](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html#sklearn.metrics.auc)
-   [metrics.confusion_matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix)

#### Visualization

-   [inspection.DecisionBoundaryDisplay](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.DecisionBoundaryDisplay.html)
-   [metrics.RocCurveDisplay](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.RocCurveDisplay.html#sklearn.metrics.RocCurveDisplay)
-   [metrics.ConfusionMatrixDisplay](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html#sklearn.metrics.ConfusionMatrixDisplay)
-   [inspection.PartialDependenceDisplay](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.PartialDependenceDisplay.html#sklearn.inspection.PartialDependenceDisplay)

#### Models

| Function                                                                                                                                                                                                                                                                                                                          | Module         | Usage                                            | Examples |
|------------------|------------------|------------------|------------------|
| [Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso) / [LassoCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html#sklearn.linear_model.LassoCV)                                                                               | `linear_model` | Fit a LASSO model with or without CV.            |          |
| [Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso) / [RidgeCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html#sklearn.linear_model.LassoCV)                                                                               | `linear_model` | Fit a Ridge model with or without CV.            |          |
| [ElasticNet](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso) / [ElasticNetCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html#sklearn.linear_model.LassoCV)                                                                     | `linear_model` | Forward an elastic net model with or without CV. |          |
| [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression)                                                                                                                                                                            | `linear_model` | Fit a linear regression model.                   |          |
| [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression) / [LogisticRegressionCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV) | `linear_model` | Fit a logistic regression model.                 |          |
| [LinearDiscriminantAnalysis](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis)                                                                                                                            | `discriminant` | Fit an LDA model.                                |          |
| [QuadraticDiscriminantAnalysis](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis)                                                                                                                   | `discriminant` | Fit a QDA model.                                 |          |
| [GaussianNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB)                                                                                                                                                                                                | `naive_bayes`  | Fit a Gaussian Naive Bayes model.                |          |
| [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier) / [tree.DecisionTreeRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor)             | `tree`         | Fit a decision tree.                             |          |
| [KNeighborsClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier) / [KNeighborsRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor)          | `neighbors`    | Construct step functions.                        |          |
|                                                                                                                                                                                                                                                                                                                                   |                |                                                  |          |

#### Preprocessing

| Function                                                                                                                                                                                    | Module              | Usage                                                      | Examples |     |     |     |     |
|---------|---------|---------|---------|---------|---------|---------|---------|
| [`FeatureUnion`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html)                                                                                      | `pipeline`          | Merge two or more sets of features.                        |          |     |     |     |     |
| [`Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)                                                                                              | `pipeline`          | Chain methods into a pipeline.                             |          |     |     |     |     |
| [SequentialFeatureSelector](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SequentialFeatureSelector.html#sklearn.feature_selection.SequentialFeatureSelector) | `feature_selection` | Forward and backward selection of features.                |          |     |     |     |     |
| [ColumnTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html#sklearn.compose.ColumnTransformer)                                             | `compose`           | Transform all feature columns.                             |          |     |     |     |     |
| [TransformedTargetRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.compose.TransformedTargetRegressor.html#sklearn.compose.TransformedTargetRegressor)                  | `compose`           | Transform target.                                          |          |     |     |     |     |
| [FunctionTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html#sklearn.preprocessing.FunctionTransformer)                           | `preprocessing`     | Transform features using custom function.                  |          |     |     |     |     |
| [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler)                                          | `preprocessing`     | Scale features to unit variance and zero mean.             |          |     |     |     |     |
| [SplineTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.SplineTransformer.html#sklearn.preprocessing.SplineTransformer)                                 | `preprocessing`     | Construct B-splines.                                       |          |     |     |     |     |
| [OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder)                                             | `preprocessing`     | Use categorical variables.                                 |          |     |     |     |     |
| [PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html#sklearn.preprocessing.PolynomialFeatures)                              | `preprocessing`     | Generate polynomial and interaction features.              |          |     |     |     |     |
| [KBinsDiscretizer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html#sklearn.preprocessing.KBinsDiscretizer)                                    | `preprocessing`     | Construct step functions.                                  |          |     |     |     |     |
| [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA)                                                                           | `decomposition`     | Construct new features using principal component analysis. |          |     |     |     |     |
