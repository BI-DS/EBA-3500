# Exercise 7.9.6 {.unnumbered}
> In this exercise, you will further analyze the `Wage` data set considered throughout this chapter.

```{python}
import pandas as pd
import numpy as np
wage = pd.read_csv("data/Wage.csv")
wage.head()
```

We only need `wage` and `age`; it's also handy to sort by `age`.

```{python}
wage.sort_values(by = "age", inplace = True)
X = wage["age"].to_numpy().reshape(-1,1)
y = wage["wage"]
```


It's always fruitful to have a look at the scatterplot before modelling the data.
```{python}
import matplotlib.pylab as plt
plt.plot(X,y,"bo")
plt.xlabel("Age")
plt.ylabel("Wage")
plt.show()
```
We see, roughly, an inverse $U$-shape with higher variance in the middle than at the extreme ends. Moreover, there are at least two separate groups in the data, a "normal" group and a group with high wages.


# With pipes

## (a)
> Perform polynomial regression to predict wage using age. Use cross-validation to select the optimal degree d for the polynomial. Make a plot of the resulting polynomial fit to the data.

```{python}
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import GridSearchCV

reg = make_pipeline(PolynomialFeatures(), LinearRegression())
grid = GridSearchCV(reg, param_grid={'polynomialfeatures__degree': range(1,10)}, cv = 5, scoring = "neg_mean_squared_error")
fit = grid.fit(X,y)
```

```{python}
plt.plot(X,y,"bx")
plt.xlabel("Age")
plt.ylabel("Wage")
plt.plot(X,fit.best_estimator_.predict(X), "r")
plt.show()
```

## (b)
> Fit a step function using to predict wage using age, and perform cross-validation to choose the optimal number of cuts. Use the quantile strategy. Make a plot of the fit obtained.

```{python}
from sklearn.preprocessing import KBinsDiscretizer
reg = make_pipeline(KBinsDiscretizer(), LinearRegression())
grid = GridSearchCV(reg, param_grid={'kbinsdiscretizer__n_bins': range(2,20)}, cv = 5, scoring = "neg_mean_squared_error")
fit = grid.fit(X,y)
```

```{python}
plt.plot(X,y,"bx")
plt.xlabel("Age")
plt.ylabel("Wage")
plt.plot(X,fit.best_estimator_.predict(X), "r")
plt.show()
```

## (c)
> Fit a step function using to predict wage using age, and perform cross-validation to choose the optimal number of cuts. Use the `uniform` strategy. Make a plot of the fit obtained.

```{python}
from sklearn.preprocessing import KBinsDiscretizer
reg = make_pipeline(KBinsDiscretizer(strategy="uniform", subsample=None), LinearRegression())
grid = GridSearchCV(reg, param_grid={'kbinsdiscretizer__n_bins': range(2,20)}, cv = 5, scoring = "neg_mean_squared_error")
fit = grid.fit(X,y)
```

```{python}
plt.plot(X,y,"bx")
plt.xlabel("Age")
plt.ylabel("Wage")
plt.plot(X,fit.best_estimator_.predict(X), "r")
plt.show()
```

# Without pipes
## (a)
> Perform polynomial regression to predict wage using age. Use cross-validation to select the optimal degree d for the polynomial. What degree was chosen, and how does this compare to the results of hypothesis testing using ANOVA? Make a plot of the resulting polynomial fit to the data.

Recall that `PolynomialFeatures` can be used to construct polynomial features. For instance, a quadratic polynomial can be found using
```{python}
from sklearn.preprocessing import PolynomialFeatures
X2 = PolynomialFeatures(2).fit_transform(x.reshape(-1,1))
X2
```

The $R^2$ for the quadratic model can be found using the `score` method.
```{python}
from sklearn.linear_model import LinearRegression as LinReg
LinReg().fit(X2, y).score(X2,y)
```
Cross-validation can be done using `cross_validate`. The default scoring parameter is the $R^2$.

```{python}
from sklearn.model_selection import cross_val_score
cv_results = cross_val_score(LinReg(), X2, y, cv=10)
r2 = cv_results.mean()
r2
```
The resulting cross-validated $R^2$ is negative; this means that the model performs worse than the mean.

To do the cross-validation for many polynomials we need a function. We will use the negative mean squared error since it is better to compare models in terms of that than the $R^2$.

```{python}
def cv_poly(degree):
  X = PolynomialFeatures(degree).fit_transform(x.reshape(-1,1))
  return cross_val_score(LinReg(), X, y, cv=10, scoring="neg_mean_squared_error").mean()
```

```{python}
cvs = {k:cv_poly(k) for k in range(0,6)}
cvs
```

```{python}
import matplotlib.pylab as plt
plt.plot(cvs.keys(), -np.array(list(cvs.values())), "bo")
plt.xlabel("Degree of polynomial")
plt.ylabel("CV mean squared error")
plt.show()
```

The optimal degree is 3. 

```{python}
X3 = PolynomialFeatures(3).fit_transform(x.reshape(-1,1))
fit = LinReg().fit(X3, y)
```

```{python}
import matplotlib.pylab as plt
plt.plot(x,y,"bo")
plt.xlabel("Age")
plt.ylabel("Wage")
plt.plot(x, fit.predict(X3), linewidth =2, color = "orange")
plt.show()
```

## (b)
> Fit a step function to predict wage using age, and perform crossvalidation to choose the optimal number of cuts. Make a plot of the ft obtained.

```{python}
from sklearn.preprocessing import KBinsDiscretizer
est = KBinsDiscretizer(
    n_bins=5, encode='onehot-dense', strategy='uniform'
)
est.fit(x.reshape(-1, 1))
X = est.transform(x.reshape(-1, 1))
fitter = LinReg().fit(X,y)
```

```{python}
```{python}
import matplotlib.pylab as plt
plt.plot(x,y,"bo")
plt.xlabel("Age")
plt.ylabel("Wage")
plt.plot(x, fitter.predict(X), linewidth =2, color = "orange")
plt.show()
```

```{python}
def cv_step(n_bins):
  est = KBinsDiscretizer(
    n_bins=n_bins, encode='onehot-dense', strategy='uniform'
  )
  est.fit(x.reshape(-1, 1))
  X = est.transform(x.reshape(-1, 1))
  return cross_val_score(LinReg(), X, y, cv=10, scoring="neg_mean_squared_error").mean()

cvs = {k:cv_step(k) for k in range(2,20)}
```


```{python}
(-np.array(list(cvs.values()))).argmin()
``` 

```{python}
import numpy as np
import matplotlib.pylab as plt
cvs = {k:cv_step(k) for k in range(2,20)}
plt.plot(cvs.keys(), -np.array(list(cvs.values())), "bo")
plt.xlabel("Number of steps")
plt.ylabel("CV mean squared error")
plt.show()
```