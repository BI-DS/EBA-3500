# Scoring rules for classifiers {.unnumbered}

* Should be one lecture.
* Can write several pages, ~20 perhaps.
* Imbalanced data.


* Scoring rules
* **Bayes classifier.** The best possible classifier given the data.
* Accuracy score.
    * As a loss: zero-one loss.
    * Best possible score given the features: "Bayes error rate", equal $P(Y=1)
    * "Irreducible error".
    * Not a proper scoring rule.
* ROC and AUC
* The log loss and cross-entropy loss.
    * Best possible loss given the features: 
    * Sometimes called logistic loss.
    * A proper loss function.
    * Has information-theoretic justification.
* Brier score or quadratic scoring rule.
    * A proper scoring rule.
    * Not as well-justified as the log loss.
* Are other proper scoring rules too, the spherical scoring rule is well-known.
* The AUC is not a proper scoring rule.
* Confusion matrices
* *Maybe* proper scoring rules.
* Decision rules and separation of concerns.
    * Make a model that represents reality as well as possible. Roughly speaking, it should have the smallest possible 
* Find some decent references.
* Log loss usually outpeforms its competitors.
    * Decision trees: Gini impurity vs log loss. Log loss wins.
    * Logistic regression: Is estimated using maximum likelihood; equivalent to minimizing the empirical log loss. This algorithm performs much better than 
* Exercises: Compare metrics,


```{python}
#| echo: False
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
```

## Scoring and loss functions
* In cross-validation.
    * `sklearn` uses the convention that higher scores are better.
    * Such scores are often called utility functions, especially in economics and philosophy.
    * Metrics with loss in their name work in the opposite way. Smaller losses are better.    

* In estimation. Logistic regression minimizes the log loss under the model $P(Y=1\mid X)=\frac{\exp{\beta^T X}}{(1+\exp(\beta^T X))}$. Neural networks 
* In the final model evaluation.

For classificaiton problems the only proper scoring rules in `sklearn` are the Brier score (`neg_brier_score`) and `neg_log_loss`. 


# The ROC curve and AUC are often used
* Discrete classifier (kNN) (is a point)
* Probabilistic classifier (all others) produces a curve.
* The ROC depicts the tradeoff between benefits (true positives) and costs (false positives).
* A conservative classifier is on the left-hand side.
* A liberal classifier is on the right-hand side.
* The line $x=y$ is a random classifier.
* Worse-than-random classifiers are below the line $x=y$. 

https://stackoverflow.com/questions/34881298/roc-curve-shows-strange-pattern


# Proper scoring rules

**Motivation:** You run a weather forecasting company. You want to provide incentives to your employees for giving the best possible forecasts in the set `rain`, `cloudy` and `sunny`.

s(p,Y)

$Es(p,Y)$

Strictly proper scoring rules: The **logarithmic scoring rule** and the **Brier score**.

Improper scoring rules: The AUC, the misclassification error, any rule that does not depend on probabilities.

Let $Y\in \{1,2,\ldots C\}$ be a qualitative target variable and $X$ be a collection of features. 

# Connection to machine learning

# Statistical efficiency

# Conditional entropy and $R^2$

$$h(Y\mid X) = -\sum_{y\in\mathcal{C}} \int \log(x \mid y)f(y,x)dx$$

$$
1-\frac{\min_{q\in\mathcal{Q}}E[l(Y,q(X))]}{\min_{q}E[l(Y,q)]},
$$

# The zoo of metrics

# Resources
As usual, none of these resources are on the curriculum. 
* https://intelligence.org/files/TechnicalExplanation.pdf

* https://www.fharrell.com/post/class-damage/

* https://academic.oup.com/jamia/article/29/9/1525/6605096?login=false

* https://people.inf.elte.hu/kiss/11dwhdm/roc.pdf

# Exercises

1. Looking at cross-validation scores.