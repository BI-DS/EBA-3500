## Exercise 4.8.14 {.unnumbered}
---
format:
  html:
    code-fold: false
    theme:
      light: sandstone
      dark: superhero
jupyter: python3
---

> In this problem, you will develop a model to predict whether a given car gets high or low gas mileage based on the `Auto` data set.

As always, we start by importing the data, with index column set to name. We also import Numpy.

```{python}
import numpy as np
import pandas as pd
auto = pd.read_csv("data/Auto.csv", index_col="name")
auto.head()
```

The `auto` data must be massaged a little more before usage. Take a look at the `info`:

```{python}
auto.info()
```

Here horsepower has type `object`; this is must be an error, because horsepower is a number, and should thus be `float64` or another numeric type.

The error is caused by missing values encoded as "?", as we can see here:

```{python}
np.unique(auto["horsepower"])
```

To remedy this, must call `read_csv` again, but with the special `na_values` argument. Moreover, we remove all rows containing `na` using the `dropna` method.

```{python}
auto = pd.read_csv("data/Auto.csv", na_values="?", index_col="name").dropna()
```

## (a)
> Create a binary variable, mpg01, that contains a 1 if mpg contains a value above its median, and a 0 if mpg contains a value below its median. You can compute the median using the median() method of the data frame. [...]

The column `mpg01` can be inserted as follows. Observe the `1*`-trick, which converts boolean values (`TRUE`/`FALSE`) into `1` and `0`.

```{python}
import numpy as np
import pandas as pd
auto["mpg01"] = 1*(auto["mpg"] > auto["mpg"].median())
```

## (b)
> Explore the data graphically in order to investigate the association between `mpg01` and the other features. Which of the other features seem most likely to be useful in predicting `mpg01`? Scatterplots and boxplots may be useful tools to answer this question. Describe your findings.

The `pairplot` function from `seaborn` is what we want. Since we're mainly interested in the association between the covariates and `mpg01`, we may use the argument `y_vars` to restrict the pairplot to these variables; but be aware that plots with categorical values tend to not very informative.
```{python}
import seaborn as sns
import matplotlib.pylab as plt
sns.pairplot(auto, y_vars = ["mpg01"])
plt.show()
```

What do we see? First, `mpg` is strongly associated with `mpg01`, but that's by definition. The other plots are hard to read. It might help to look at a correlation matrix instead.

```{python}
auto.corr()
```
Now we see a sizeable correlation between `mpg01` and most covariates, with the relatively high `0.32` for acceleration being the highest. Also observe that `cylinders`, `weight`,  `horsepower`, and `displacement` are highly correlated. Recall that we call this *multicollinearity*, and it's best not to use all of them in a single model.

## (c)
> Split the data into a training set and a test set.

This is most easily done using `train_test_split`. But first we need to remove every covariate we don't need from `auto`. This includes `mpg0`, `mpg`, and `name`.

```{python}
from sklearn.model_selection import train_test_split
X = auto.drop(["mpg01", "mpg"], axis = 1)
y = auto["mpg01"]
X_train, X_test, y_train, y_test = train_test_split(X,y, 
                                   random_state=313,  
                                   test_size=0.20,  
                                   shuffle=True) 

```

## (d)
>  Perform LDA on the training data in order to predict `mpg01` using the variables that seemed most associated with `mpg01` in (b). What is the test error of the model obtained?

Due to high multicollinearity we go only with `cylinders`, not `cylinders`, `weight`,  `horsepower`, and `displacement`. We also include `acceleration`, `origin`, and `year` due to their clear associations. *There are many valid solutions to this exercise! You need to show that you understand how to select reasonable variables and that you know about multicollinearity.*

```{python}
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
features = ["cylinders", "acceleration", "origin", "year"]
fit = LDA().fit(X_train[features], y_train)
```

We find the test error using the `score` method.

```{python}
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
fit = LDA().fit(X_train[features], y_train)
LDA_score = fit.score(X_test[features], y_test)
LDA_score
```

## (e)
> Perform QDA on the training data in order to predict `mpg01` using the variables that seemed most associated with mpg01 in (b). What is the test error of the model obtained?

We do the same as last time, but with QDA instead of LDA. 
```{python}
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA
fit = QDA().fit(X_train[features], y_train)
QDA_score = fit.score(X_test[features], y_test)
QDA_score
```

The QDA score is better than the LDA score.

## (f)
> Perform logistic regression on the training data in order to predict mpg01 using the variables that seemed most associated with `mpg01` in (b). What is the test error of the model obtained?

```{python}
from sklearn.linear_model import LogisticRegression as Logit
fit = Logit().fit(X_train[features], y_train)
logit_score = fit.score(X_test[features], y_test)
logit_score
```

## (g)
> Perform logistic regression on the training data in order to predict mpg01 using the variables that seemed most associated with `mpg01` in (b). What is the test error of the model obtained?

```{python}
from sklearn.naive_bayes import GaussianNB as NB
fit = NB().fit(X_train[features], y_train)
nb_score = fit.score(X_test[features], y_test)
nb_score
```

## (h)
```{python}
from sklearn.neighbors import KNeighborsClassifier as KNN
knn_test_scores = [KNN(k).fit(X_train[features], y_train).score(X_test[features], y_test) for k in range(1,40)]
```

Let's plot the `knn_scores`; see Figure 2.17 in the book. To do this we need the training error too:

```{python}
from sklearn.neighbors import KNeighborsClassifier as KNN
knn_train_scores = [KNN(k).fit(X_train[features], y_train).score(X_train[features], y_train) for k in range(1,40)]
```

Then we can plot a similar plot as Figure 2.17. This plot demonstrates the difference between training and test error and allows us to choose a reasonable value for $k$.

```{python}
import matplotlib.pylab as plt
plt.clf()
plt.plot(np.arange(1,40), knn_test_scores, label="Test error")
plt.plot(np.arange(1,40), knn_train_scores, label="Training error")
plt.xlabel("k parameter")
plt.ylabel("Accuracy score (higher is better)")
plt.legend(loc="upper right")
plt.show()
```

It appears that $k$ between $\approx15$ and $\approx20$ would work well. The accuracy for $k=16$ is
```{python}
knn_score = knn_test_scores[16]
knn_score
```

In conclusion we get the following scores:

| Method | Score |
| -----  | ----  |
| LDA    | `{python} round(LDA_score,3)` |
| QDA    | `{python} round(QDA_score,3)` |
| Logistic | `{python} round(logit_score,3)` |
| Naive Bayes | `{python} round(nb_score,3)` |
| KNN(16) | `{python} round(knn_score,3)` |
