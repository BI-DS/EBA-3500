---
title: "Solution to exercise 8.7"
author: Jonas Moss
date: last-modified
execute:
  echo: true
jupyter: python3
reference-location: margin
citation-location: margin
---

> In Section 8.3.3, we applied random forests to the Boston data using `max_features = 6` and using `n_estimators = 100` and `n_estimators = 500`. Create a plot displaying the test error resulting from random forests on this data set for a more comprehensive range of values for `max_features` and `n_estimators`. You can model your plot after Figure 8.10. Describe the results obtained.

```{python}
#| echo: False
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
boston = pd.read_csv("data/Boston.csv", index_col=0)
```

```{python}
from sklearn.model_selection import train_test_split

random_state = 313
X = boston.drop(["crim"], axis=1)
y = boston["crim"]
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=random_state
)

```

## (a) Random forest

Import `RandomForestRegressor()`. 

```{python}
rand = 313
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error as mse

regr = RandomForestRegressor(random_state=rand)
regr.fit(X_train, y_train)
regr.score(X_test, y_test)
mse(regr.predict(X_test), y_test)
```

```{python}
regr_n500 = RandomForestRegressor(n_estimators=500, random_state=rand)
regr_n500.fit(X_train, y_train)
regr_n500.score(X_test, y_test)
mse(regr_n500.predict(X_test), y_test)
```

```{python}
regr_fsqrt = RandomForestRegressor(max_features="sqrt", random_state=rand)
regr_fsqrt.fit(X_train, y_train)
regr_fsqrt.score(X_test, y_test)
mse(regr_fsqrt.predict(X_test), y_test)
```

```{python}
regr_flog2 = RandomForestRegressor(max_features="log2", random_state=rand)
regr_flog2.fit(X_train, y_train)
regr_flog2.score(X_test, y_test)
mse(regr_flog2.predict(X_test), y_test)
```



```{python}
importance = pd.DataFrame(
    {"Importance": regr_fsqrt.feature_importances_}, index=X_train.columns
)
importance.sort_values(by="Importance", ascending=False).style.format(
    {"Importance": "{:.2g}"}
).background_gradient(cmap="coolwarm")
```

(The final estimate is highly variable; is this even useful?)

## (b) Gradient boosting

```{python}
from sklearn.ensemble import GradientBoostingRegressor

regr_boost = GradientBoostingRegressor(random_state=rand)
regr_boost.fit(X_train, y_train)
mse(regr.predict(X_test), y_test)
```


```{python}
regr_boost_big = GradientBoostingRegressor(
    random_state=rand, n_estimators=5000, max_depth=3
)
regr_boost_big.fit(X_train, y_train)
mse(regr_boost_big.predict(X_test), y_test)
```

```{python}
regr_boost_big = GradientBoostingRegressor(
    learning_rate=0.001, random_state=rand, n_estimators=5000, max_depth=3
)
regr_boost_big.fit(X_train, y_train)
mse(regr_boost_big.predict(X_test), y_test)
```



## (c) AdaBoost

```{python}
from sklearn.ensemble import AdaBoostRegressor

regr_ada = AdaBoostRegressor(random_state=rand)
regr_ada.fit(X_train, y_train)
mse(regr_ada.predict(X_test), y_test)
```

```{python}
from sklearn.ensemble import AdaBoostRegressor

regr_ada_small_lr = AdaBoostRegressor(random_state=rand, learning_rate=0.001)
regr_ada_small_lr.fit(X_train, y_train)
mse(regr_ada_small_lr.predict(X_test), y_test)
```