{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exercise 4.6.9 {.unnumbered}\n",
        "\n",
        "> In this exercise, we will predict the number of applications received using the other variables in the College data set.\n",
        "\n",
        "Let's import `College`. The argument `index_col=0` tells `pandas` to view the first column as an index column; that's appropriate, as can be seen below.\n"
      ],
      "id": "520a0d6c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "college = pd.read_csv(\"data/College.csv\", index_col = 0)\n",
        "college.head()"
      ],
      "id": "776fcb26",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the `info` method we see that all columns except \"Private\" has a useful format. Before proceeding, we change `Private` into `int64`. \n"
      ],
      "id": "a53ca225"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "college[\"Private\"].replace({\"Yes\": 1, \"No\": 0}, inplace = True)\n",
        "college.info()"
      ],
      "id": "f5b7324b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, since we are working with ridge and lasso in this exercise, we need to\n",
        "scale the input. We want the mean of each column to be $0$ and its variance to be $1$. \n",
        "\n",
        "Now, however, we have the variances\n"
      ],
      "id": "d7c6ffe3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "college.var()"
      ],
      "id": "12f39d2a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll use the `StandardScaler` along with Pipeline to scale the data. It's also possible to scale the data manually, as done in the lab. (The lab also mentions the pipeline and scaler though.)\n",
        "\n",
        "## (a)\n",
        "> Split the data set into a training set and a test set.\n",
        "\n",
        "We have done this many times; use the `train_test_split` function for this.\n"
      ],
      "id": "b66e07d7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = college.drop([\"Apps\", \"Accept\"], axis=1)\n",
        "y = college[\"Accept\"] / college[\"Apps\"]\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, random_state=1, test_size=0.20, shuffle=True\n",
        ") "
      ],
      "id": "f7f2c666",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## (b)\n",
        "> Fit a linear model using least squares on the training set, and report the test error obtained.\n",
        "\n",
        "A linear regression model can be trained using the `LinearRegression` class from `sklearn.linear_model`. The exercise is not $100\\%$ clear about what test error means, so we will go for the mean squared error. (Recall that smaller mean squared errors are better.)\n"
      ],
      "id": "79b10fdc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "linreg = Pipeline(steps=[(\"scale\", StandardScaler()), (\"reg\", LinearRegression())])\n",
        "linreg.fit(X_train, y_train)\n",
        "linreg_mse = mean_squared_error(y_true=y_test, y_pred=linreg.predict(X_test))\n",
        "linreg_mse"
      ],
      "id": "e1f4dba7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Alternatively, we could go for the $R^2$, which is easier to compute and easier to interpret than the mean squared error. The mean squared error is better suited for model comparisons though."
      ],
      "id": "3cfb1e0f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "linreg.score(X_test,y_test)"
      ],
      "id": "0ad6f43a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## (c)\n",
        "> Fit a ridge regression model on the training set, with $\\lambda$ chosen by cross-validation. Report the test error obtained.\n",
        "\n",
        "The $\\lambda$ parameter is called $\\alpha$ in [sklearn](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression-and-classification). Cross-validation is done by `RidgeCV`, so we do not have to implement it ourselves. [`RidgeCV`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html#sklearn.linear_model.RidgeCV) defaults to leave-one-out cross-validation since it is fast, but other options are possible.\n"
      ],
      "id": "e1ae0e39"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.linear_model import RidgeCV\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import numpy as np\n",
        "ridge = make_pipeline(StandardScaler(),RidgeCV(cv=5))\n",
        "ridge.fit(X_train, y_train)"
      ],
      "id": "6b4608bd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ridge_mse = mean_squared_error(y_true=y_test, y_pred=ridge.predict(X_test))\n",
        "ridge_mse "
      ],
      "id": "1158db59",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## (d)\n",
        "> Fit a lasso model on the training set, with $\\lambda$ chosen by cross-validation. Report the test error obtained, along with the number of non-zero coefcient estimates.\n",
        "\n",
        "We'll use `LassoCV` along with 5-fold cross-validation.\n"
      ],
      "id": "6af2ac8e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.linear_model import LassoCV\n",
        "lasso = make_pipeline(StandardScaler(),LassoCV(cv=5, random_state=0))\n",
        "lasso.fit(X_train, y_train)"
      ],
      "id": "597d0c4d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we find the mean squared error."
      ],
      "id": "9f5eba48"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "lasso_mse = mean_squared_error(y_true=y_test, y_pred=lasso.predict(X_test))\n",
        "lasso_mse "
      ],
      "id": "73e93cd1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The mean squared error is much worse than for ridge and linear regression. This is not unusual: Lasso is typically worse at prediction problems than ridge, but is better at finding a small set of decent predictors.\n",
        "\n",
        "The list of coefficient is in the list `lasso.namesd_steps[\"lassocv\"].coef_` (found by `lassocv.coef_` if fitted without a pipeline). The list is not named though, so to identify the non-zero coefficient we can construct a data frame from it."
      ],
      "id": "9c77f835"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "coefs = pd.DataFrame({\"coef\": lasso.named_steps[\"lassocv\"].coef_}).set_index(\n",
        "    X_test.columns\n",
        ")\n",
        "coefs"
      ],
      "id": "4a321c63",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can filter out the non-zero coefficient"
      ],
      "id": "c57e4743"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "coefs[coefs[\"coef\"] != 0].sort_values(\n",
        "    \"coef\", key=np.abs, ascending=False\n",
        ").style.background_gradient()"
      ],
      "id": "2ac8007d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## (e)\n",
        "> Train and elastic net model instead.\n"
      ],
      "id": "950d5d03"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.linear_model import ElasticNetCV\n",
        "elasticnet = make_pipeline(StandardScaler(),ElasticNetCV(cv=5, random_state=0))\n",
        "elasticnet.fit(X_train, y_train)"
      ],
      "id": "268ca37e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "elasticnet_mse = mean_squared_error(y_true=y_test, y_pred=elasticnet.predict(X_test))\n",
        "elasticnet_mse"
      ],
      "id": "3fdd45d8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## (g)\n",
        "> Comment on the results obtained. How accurately can we predict the number of college applications received? Is there much diference among the test errors resulting from these fve approaches?\n"
      ],
      "id": "6652accd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pd.DataFrame(\n",
        "    {\n",
        "        \"MSE\": [linreg_mse, ridge_mse, lasso_mse, elasticnet_mse],\n",
        "        \"R2\": [\n",
        "            linreg.score(X_test, y_test),\n",
        "            ridge.score(X_test, y_test),\n",
        "            lasso.score(X_test, y_test),\n",
        "            elasticnet.score(X_test, y_test),\n",
        "        ],\n",
        "    },\n",
        "    index=[\"ols\", \"ridge\", \"lasso\", \"elastic_net\"],\n",
        ").style.format(precision=4)"
      ],
      "id": "e53681cf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## (h)\n",
        "> Transform the regressor using the logit function.\n"
      ],
      "id": "7eb91c00"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.linear_model import RidgeCV\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.compose import TransformedTargetRegressor\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "inverse_func = lambda x: 1 / (1 + np.exp(-x))\n",
        "func = lambda p: np.log(p) - np.log(1 - p)\n",
        "\n",
        "ridge = make_pipeline(\n",
        "    StandardScaler(),\n",
        "    TransformedTargetRegressor(\n",
        "        regressor=RidgeCV(cv=5), func=func, inverse_func=inverse_func\n",
        "    ),\n",
        ")\n",
        "ridge.fit(X_train, y_train)\n",
        "ridge_mse = mean_squared_error(y_true=y_test, y_pred=ridge.predict(X_test))\n",
        "ridge_mse "
      ],
      "id": "6be49e30",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}