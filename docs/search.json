[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data analysis with Programming",
    "section": "",
    "text": "Introduction\nWelcome to the course Data Analysis with Programming. This page contains the curriculum, exercises, and additional information for the course. If you need to get in contact with me, please send an e-mail to jonas.moss@bi.no. I do not check It’s learning often.\nThe majority of our curriculum is covered by two books.\nDekking, F. M., Kraaikamp, C., Lopuhaä, H. P., & Meester, L. E. (2005). A Modern Introduction to Probability and Statistics. Springer London. https://doi.org/10.1007/1-84628-168-7\nYou should know this book from your previous course in probability. We will follow the book closely.\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An Introduction to Statistical Learning. Springer US. https://doi.org/10.1007/978-1-0716-1418-1\nThis one is available online for free; its webpage is here. It is a classic, and you can expect other data scientists to know it well. It is a simplified version Elements of Statistical Learning by Hastie, Tibshirani and Friedman, the best known and most widely referenced book of machine learning. The ambitious student will read Elements of Statistical Learning in addition to Introduction to Statistical Learning, but it is a significantly more difficult book.\nBe warned that Dekking et. al covers no programming at all, and James et al. uses R instead of Python. This will not cause us much difficulty, as R and Python are very similar languages. Supplementary material about Python can be found in the sidebar.\nThis course follows no strict schedule, as it is based around video lectures. The topics along with reading materials and associated video lectures can be found on the bar to the left. You will also find exercises there.\nThis course is partly about programming. Students often find programming hard. Don’t expect to be able solve every exercise in 5 minutes! Solving programming exercises often take a long time, and you need to persevere. The only way to become good at programming requires you to invest a lot of time, despite the many empty promises out there. I would recommend you read the short Teach Yourself Programming in Ten Years by famous AI researcher Peter Norvig.\nTo become a decent programmer it’s a good idea to\nDo not to spend an inordinate amount of time on an exercise before you check the solution. If you have spent 1 hour on an exercise and haven’t gotten anywhere, it might be smart to save yourself some time and look at the solution. As I said, you can always come back to it later.\nMoreover, be aware that programming is often extremely frustrating. It’s like talking to someone who just simply refuses to understand what you’re saying, no matter how many times you repeat yourself. It’s normal and expected to feel frustrated!\nThere are many tips online about learning to program, e.g., this collection of tips. But it mostly boils down to spending a lot of time solving problems.\nCurious how this site was made? It is written using Quarto books."
  },
  {
    "objectID": "index.html#do-the-work",
    "href": "index.html#do-the-work",
    "title": "Data analysis with Programming",
    "section": "Do the work!",
    "text": "Do the work!\nThis is not a relaxing course. The content is difficult, both in terms of concepts and skills required. Statistics has an unfair and completely untrue reputation as an easy subject. It is not. Those who claim statistics is easy do not know statistics.\nRemember that you are expected to work full days as a student. Since you take four courses and all of them, presumably, have approximately \\(2\\) hours of lectures, that leaves \\(8\\) hours of studying – on your own – each week. I expect you to spend at least \\(10\\) hours with this course per week. But keeping in mind that this course is both significantly more important and significantly harder than your other courses, so you might have to spend even more time on it."
  },
  {
    "objectID": "index.html#resources",
    "href": "index.html#resources",
    "title": "Data analysis with Programming",
    "section": "Resources",
    "text": "Resources\nYou should probably use other resources than the lecture videos, the books, and the exercises. That is not because these resources do not cover the curriculum. It is because there are 100s of ways to teach the curriculum, and just as many points of view on the difficult parts. You often need to spend significant amounts of time with a concept, looking at it from different angles, in order to finally grok it. I did this all the time as a student, and every successful data scientist I know does this.\nYou should search the internet for answers early on. Understanding what to search for is an extremely important skill for life in general, but especially for data science, programming, and statistics. The key sites to look at are\n\nStackOverflow The primary resource for programming questions. Often covers statistics and data science questions too. Do not be afraid to ask questions there. You might get mean-spirited answers though. (Just accept the mean-spirited answers and move on.)\nCrossValidated The most widely used statistics Q&A site. Most answers are trustworthy, but the answers are often of lower quality than the other pages, and sometimes wrong. Ignore answers that look iffy.\nMathematics Stack Exchange You use this for math questions. Probably not that useful in this course, but it might come up.\n\nThere is no definite 1st course in Python curriculum, which is a big and somewhat unwieldy language. You might not have learned about classes (object-oriented Python) or list comprehensions, or maybe not dictionaries either. I will use these concepts without additional explanation. Hence I would strongly recommend you spend 4 hours or so getting reasonably familiar with object-oriented programming in Python. It is not strictly speaking necessary for this course, as you won’t be asked to write your own classes. But some understanding of classes, methods, and attributes will help you understand what’s going on, perhaps even to a great degree! A reasonable place to start would be the videos of freeCodeCamp."
  },
  {
    "objectID": "site/00-python-tips.html#professional-programming",
    "href": "site/00-python-tips.html#professional-programming",
    "title": "Python tips",
    "section": "Professional programming",
    "text": "Professional programming\n\nFollow the style guide PEP.\nUse automatic formatters. I would recommend black, as it doesn’t give you any options.\nDocument your functions using docstrings.\n\nFollow the Python conventions. You may also want to follow more detailed guidelines such as the Google style guide.\nAt some point you want to learn about type hints. These make your code easier to understand and debug.\n\nTry to look into testing frameworks.\nUse modules to organize your work.\nLearn to use Github (with their student pack). You want to use Github due to its versions control and since it allows you to collaborate easily.\n\n\nHow?\n\nYou should try to apply your recently learned knowledge as often as possible. Have an assignment to write a Python function? Format it using black, write a docstring for it, use typehints, put it into an appropriate module (such as an EBA3500 module), give it a proper name, and publish it on your Github page. You can also practice some of these skills using gamified sites such as Codewars.\nAnother option is to work on random small projects, clean them up, and publish them on Github. Perhaps together with some of your costudents?"
  },
  {
    "objectID": "site/00-python-tips.html#better-programming",
    "href": "site/00-python-tips.html#better-programming",
    "title": "Python tips",
    "section": "Better programming",
    "text": "Better programming\n\nObject-oriented programming and classes. The first priority is to learn about classes and object-oriented programming. Most Python libraries for statistics (statsmodels) and machine learning (scikit-learn, PyTorch, Keras) Python are built around classes.\nLearn the features of the language.\n\nMake sure you know the basic data structures of the language.\nSkim the standard library and become familiar with the, for us at least, most important parts; collections, itertools, (and, perhaps string). There are many exercises on e.g. Codewars (exercises ranked as 7kyu and 8kyu) that will help you with this.\nLearn more advanced language features, such as generators, the pattern matching operator match, context managers for handling files and connections (using the with statement), and nested list comprehensions.\nLearn about common pitfalls. For instance, be careful when removing elements from a list in a for loop, and be aware that functions can modify their input.\n\nAlgorithms and data structures. Algorithms and data structures are about understanding how much time and memory programs require. This is a big field, but some reasonable goals is to\n\nUnderstand the difference between \\(O(n)\\), \\(O(n^2)\\), and \\(O(n\\log n)\\) algorithms. In addition, understand what an exponential time algorithm is and why it’s important to avoid them as much as you can. Also learn about why the dictionary is far faster than a list when doing lookups.\nThe highly competetive US job interviews at top tech companies such as Facebook, Google, Apple, Microsoft and Amazon are based on algorithms and data structures. This has created an enormous market for algorithms and data structure resources online, both free and paid. Algoexpert is an example of a partly free site with excellent resources.\nExercises at Codewars at rank about 5kyu and 6kyu cover intermediate algorithmic thinking.\n\n\n\nHow?\nGetting good at programming requires time, hard work, and perseverance. First, deliberate practice: You need to practice consistently and for non-trivial amounts of time, e.g. 30 minutes per day. You might even want to join the 100DaysOfCode challenge! Second, do more than required on assignments. Play around! Perhaps your assignment could be solved with sets instead of lists? Try it! Finally, read and try to answer Stack Overflow questions."
  },
  {
    "objectID": "site/01-scipy-numpy.html#curriculum",
    "href": "site/01-scipy-numpy.html#curriculum",
    "title": "1  Introduction to Numpy",
    "section": "1.1 Curriculum",
    "text": "1.1 Curriculum\n\nNumpy for absolute beginners\nThe notes on this webpage.\n\n\n1.1.1 The speed of Numpy\nPython is very slow language. So slow, in fact, that most optimizations in Python is about moving as many computations as possible to Numpy.\nThe following function sums up all numbers from \\(1..n\\) in vanilla Python.\n\ndef sum_python(n):\n  acc = 0\n  for i in range(0, n):\n    acc = acc + i\n  return acc\n\nMake sure you understand what this function does. Can you rewrite it to calculate the product of the \\(n\\) first numbers?\nThe function below uses Numpy for the same task.\n\nimport numpy as np\ndef sum_numpy(n):\n  numbers = np.arange(0, n, dtype = np.int64)\n  return numbers.sum()\n\nHere numbers is an array of integers (int64 is a type of integer), and sum is one of many Numpy methods that operate on arrays. Unlike Python lists, Numpy arrays are typed, and this is a major reason why Numpy is so faster than Python. A Python list can contain just about anything.\n\nls = [1, \"hello\", print]\n\nHere ls contains a number, a string, and a function (actually, a “builtin function”). Recall that you can check the type of an object by calling type on it. For instance, we can verify the types above by calling\n\nprint([type(x) for x in ls])\n\n[<class 'int'>, <class 'str'>, <class 'builtin_function_or_method'>]\n\n\nLet’s come back to the matter at hand. We have two implementations of a function that sums the numbers from 1..n. Both functions return the same value.\n\nsum_python(10 ** 6)\n\n499999500000\n\n\n\nsum_numpy(10 ** 6)\n\n499999500000\n\n\nObserve that the Numpy code is arguably easier to read. There is no doubt what the .sum method does.\nWe use the dtype = np.int64 argument in the np.arange function. This makes int64 the data type of the resulting Numpy array. These are 64 bits (signed) integers, but the standard is 32 bits integer. The difference between these lie in their maximum and minimum values. The maximal value of a 64 bits integer is 9,223,372,036,854,775,807, but the maximal value of an i32 is merely 2,147,483,647. You have to manually specify i64 when dealing with big integers in Numpy, but you do not need to do that in Python, as it can use integers of arbitrary size, at the cost of speed. You can always find the data type of a Numpy object using the .dtype method, e.g.,\n\nx = np.arange(0, 10)\nx.dtype\n\ndtype('int32')\n\n\nWe compare the execution speed of these functions using the benchmark function from the tinybench package. As always, type help(benchmark) in a Python interpreter to get help for the function. Below, we sample ntimes = 10 and use a warm up of 10 (to get the processor running). The g argument tells benchmark where to find the functions in the list, and the argument globals() tells it to look at the top level.\n\nfrom tinybench import benchmark\nbench = benchmark(['sum_python(10 ** 6)', 'sum_numpy(10 ** 6)'], ntimes = 100, warmup = 10, g = globals())\nbench.plot()\n\n\n\n\nThe Numpy version is much faster. To pinpoint by exactly how much, we need to look at the mean execution times.\n\nbench.means\nbench.means['sum_python'] / bench.means['sum_numpy']\n\n13.487322159338687\n\n\nThe Numpy implementation is roughly \\(10\\) times faster. One can expect speedups even larger than this in more complex applications."
  },
  {
    "objectID": "site/01-scipy-numpy.html#exercises",
    "href": "site/01-scipy-numpy.html#exercises",
    "title": "1  Introduction to Numpy",
    "section": "1.2 Exercises",
    "text": "1.2 Exercises\n\n1.2.1 Data types\n\n1.2.1.1 Exercise 1\nFigure out the answer the following questions, using e.g. the Numpy documentation. (Look up the functions iinfo).\n\nWhat is the minimal value of a 32 bit integer in Numpy?\nWhat is the minimal value of a 64 bit integer in Numpy?\nIs there an integer type even larger than int64, provided you restrict yourself to non-negative numbers, i.e., unsigned integers?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe first two questions can be answered by\n\nnp.iinfo(np.int32)\n\niinfo(min=-2147483648, max=2147483647, dtype=int32)\n\n\n\nnp.iinfo(np.int64)\n\niinfo(min=-9223372036854775808, max=9223372036854775807, dtype=int64)\n\n\nFor the last question: Yes, the unsigned integers uint64 are larger.\n\nnp.iinfo(np.uint64)\n\niinfo(min=0, max=18446744073709551615, dtype=uint64)\n\n\n\n\n\n\n\n1.2.1.2 Exercise 2\nDecimal numbers in computer science are called floats, or floating point numbers. (Look up the functions finfo).\n\nWhat types of floats are available in Numpy?\nWhat is the default float type when using linspace?\nWhat are the maximal and minimal values of these float types?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nWe have np.float16, np.float32 and np.float64.\nUsing np.linspace(0, 1, 4).dtype we find that float64 is the default data type.\nWe can read that from the results below:\n\n\nnp.finfo(np.float64)\n\nfinfo(resolution=1e-15, min=-1.7976931348623157e+308, max=1.7976931348623157e+308, dtype=float64)\n\n\n\n\n\n\n\n\n1.2.2 Benchmarking and Numpy\n\n1.2.2.1 Exercise 1\nPython implements a method sum that sums every member of an iterable such as list. Implement a function sum_python2 that uses sum instead of a for-loop, but does not use Numpy. Compare its performance to sum_python above. What do you see?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndef sum_python2(n):\n  numbers = range(0, n)\n  return sum(numbers)\n\nfrom tinybench import benchmark, benchmark_env\nbench = benchmark(['sum_python(10 ** 6)', 'sum_python2(10 ** 6)', 'sum_numpy(10 ** 6)'], ntimes = 100, warmup = 10, g = globals())\nbench.plot()  \n\n\n\n\n\n\n\n\n\n1.2.2.2 Exercise 2\nWrite a function that squares and sums the numbers from 1 to n, one in Numpy and one in pure Python. Roughly how much faster is the Numpy implementation than the Python implementation when using n=1000, n=10000, or n=10**6? (Hint: Make sure to use a Numpy function to square all the elements!)\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndef sum_sq_python(n):\n  numbers = range(0, n)\n  acc = 0\n  for i in numbers:\n    acc = acc + i**2\n  return acc\n\ndef sum_sq_numpy(n):\n  numbers = np.arange(0, n, dtype = np.int64)\n  return np.square(numbers).sum()\n\nfrom tinybench import benchmark, benchmark_env\nbench = benchmark(['sum_sq_python(10 ** 6)', 'sum_sq_numpy(10 ** 6)'], ntimes = 100, warmup = 10, g = globals())\nbench.plot()  \n\n\n\n\n\nbench.means['sum_sq_python'] / bench.means['sum_sq_numpy']\n\n37.42279096704778\n\n\n\n\n\n\n\n1.2.2.3 Exercise 3\nRecall that the sample variance is defined as \\(\\sum (x_i - \\overline{x})^2 / (n-1)\\), where \\(\\overline{x}\\) is the sample mean and \\(n\\) is the number of observations. Compare a Numpy-free implantation to the var method of Numpy (using the optional argument ddof = 1.), on the numbers from 1 .. 10 ** 5, but this time, let dtype = float64. Be sure to check that your functions return the same result!\n(Note: You algorithm and Numpy might give different results for very large n. This is due to a phenomenon called numerical instability, which we ignore in this course.)\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndef var_python(n):\n  numbers = range(0, n)\n  mean = sum(numbers) / n\n  return sum([(x - mean) ** 2 for x in numbers]) / (n - 1)\n\ndef var_numpy(n):\n  numbers = np.arange(0, n, dtype = np.float64)\n  return numbers.var(ddof = 1)\n\n\nvar_numpy(10**5)\n\n833341666.6666666\n\n\n\nvar_python(10**5)\n\n833341666.6666666\n\n\n\n\n\n\n\n\n1.2.3 Numpy exercises\n\n1.2.3.1 Exercise 1\nHow do you do the following in Numpy? Make sure to make your own example in Python! 1. Make an identity matrix with n rows? 2. Make a matrix consisting of \\(0\\)s only? 3. Calculate the empirical mean of a vector? 4. Calculate the standard deviation of a vector normalized so that the variance is unbiased? (Hint: Read the Numpy documentation to find out what I mean!) Which of these do you think we are most interested in in this course? 5. Calculate “cumulative sum” operation on a vector x? (This operation creates a new vector y whose first element is x[0], second x[0] + x[1], etc.) 6. Take the element-wise logarithm of a matrix?\n\n\n1.2.3.2 Exercise 2\nDo this Codewars exercise using Numpy indexing, i.e., not the built-in function transpose. You need to register at Codewars to do this exercise. In this and other Codewars problems, you can click the “Unlock solutions” button for solutions.\n\n\n1.2.3.3 Exercise 3\nDo the following exercise, both with and without Numpy. (Hint: Use the round method to round the Numpy arrays. Remember to convert between list and array types!)\n\n\n1.2.3.4 Exercise 4\nDo the following exercise, both with and without Numpy.\n\n\n1.2.3.5 Exercise 5\nThe Github repo Numpy-100 contains 100 Numpy exercises of variable difficulty. The Github page also includes hints and solutions. You can read the 100 problems here. Do exercises 1 - 11 in this repo. There is no upper limit to how many of the numpy-100 you should do, but I would recommend you do as many as you can find time for. (These exercises are pretty short!) The point is to grok Numpy. (Hint: Use the Numpy documentation, Google, StackExchange, and so on. Check the hints if you have to.)"
  },
  {
    "objectID": "site/02-statistical-simulation.html#curriculum",
    "href": "site/02-statistical-simulation.html#curriculum",
    "title": "2  Statistical simulation",
    "section": "2.1 Curriculum",
    "text": "2.1 Curriculum\n\n2.1.1 Core readings\n\nThe notes below.\n(Optional) Dekking et al., Chapter 6: Simulations. Except Chapter 6.4 The single-server queue.\n\n\n\n2.1.2 Random number generators\nWe start by defining an random number generator (rng). Computers do not usually generate truly random numbers. Instead they generate so-called pseudo-random numbers using methods such as the linear congruential generator. (It is possible to generate truly random numbers using the RDRAND instruction on x64 processors, but this feature is almost never used for various reasons.)\n\nimport numpy as np\nrng = np.random.default_rng(seed = 313)\nrng\n\nGenerator(PCG64) at 0x247B8D7FBC0\n\n\nAs you can see, the object rng is a Generator(PCG64), i.e., a random number generator.\nGenerator objects have methods, such as uniform and normal, that may be used to generate random values.\n\nrng.uniform(0, 1, size = 10)\n\narray([0.63182242, 0.46427464, 0.77927765, 0.40253182, 0.61196237,\n       0.32392294, 0.10567386, 0.68671495, 0.01786417, 0.60813899])\n\n\nThis code generates a column vector of ten elements randomly sampled from the uniform distribution on \\([0,1]\\). The size argument is a Numpy dimension, hence you can write:\n\nrng.uniform(0, 1, size = (2, 10))\n\narray([[0.77322932, 0.23260444, 0.56660324, 0.96823875, 0.93026402,\n        0.71578432, 0.29358623, 0.53439132, 0.81535993, 0.42267216],\n       [0.96857697, 0.96626312, 0.24506702, 0.44363894, 0.45995697,\n        0.86179148, 0.45618709, 0.90139015, 0.51322552, 0.93442797]])\n\n\nThis is an array with \\(2\\) rows and \\(10\\) columns.\nOur generator rng was defined using the argument seed = 313. This is used for reproducibility. If you run the same code twice with the same seed, the result is going to be the same.\n\nrng1 = np.random.default_rng(seed = 313)\nrng2 = np.random.default_rng(seed = 313)\n(rng1.uniform(0, 1, size = (2, 2)), rng2.uniform(0, 1, size = (2, 2)))\n\n(array([[0.63182242, 0.46427464],\n        [0.77927765, 0.40253182]]),\n array([[0.63182242, 0.46427464],\n        [0.77927765, 0.40253182]]))\n\n\nBut the results will not be the same if we do not provide the seed!\n\nrng1 = np.random.default_rng()\nrng2 = np.random.default_rng()\n(rng1.uniform(0, 1, size = (2, 2)), rng2.uniform(0, 1, size = (2, 2)))\n\n(array([[0.29638707, 0.19384254],\n        [0.45017782, 0.98514409]]),\n array([[0.21254057, 0.8582812 ],\n        [0.40646138, 0.00169709]]))\n\n\nReproducibility is important in scientific applications, as the reader of your work can exactly reproduce your simulations on his own computer. It’s also relevant for our coursework, as it allows your teacher to automatically grade your submissions.\nThe first two arguments of rng.uniform specify the start point and end point of the interval we sample from.\n\nrng1 = np.random.default_rng(seed = 313)\nrng2 = np.random.default_rng(seed = 313)\nx = rng1.uniform(2, 5, size = (2, 2)) # starting at 2 and ending at 5,\ny = rng2.uniform(0, 1, size = (2, 2)) # starting at 0 and ending at 1.\n\n(x, 3*y + 2)\n\n(array([[3.89546727, 3.39282392],\n        [4.33783294, 3.20759547]]),\n array([[3.89546727, 3.39282392],\n        [4.33783294, 3.20759547]]))\n\n\nNow \\(x\\) and \\(y\\) are the same. This will always be the case.\n\n2.1.2.1 Quick exercise\nGenerate an array of uniformly distributed numbers on \\([1,5]\\) with \\(3\\) rows and \\(4\\) columns.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nx = rng1.uniform(2, 5, size = (3, 4))\nx\n\narray([[3.8358871 , 2.97176881, 2.31702157, 4.06014485],\n       [2.05359251, 3.82441698, 4.31968797, 2.69781331],\n       [3.69980971, 4.90471626, 4.79079206, 4.14735295]])\n\n\n\n\n\n\n\n\n2.1.3 Using distributions\nYou can generate normally distributed random values with mean mu and standard deviation sigma using rng.normal(mu, sigma, size). Again, the size argument tells numpy how many rows, columns and potentially more dimensions you want your array to have.\n\nx = rng1.normal(0, 1, 10000)\n\nLet’s verify that x is normally distributed by plotting its histogram.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.clf()\nsns.histplot(x, stat = \"density\")\nplt.show()\n\n\n\n\nRecall the formula for the density of the normal distribution, \\[f(x;\\mu,\\sigma) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp{\\lbrack-\\frac{1}{2}(x-\\mu)^2}\\rbrack.\\] To verify that the histogram is normal, we can overlay the density of a normal on top of it.\n\ny = np.linspace(-5, 5, 1000)\ndef normpdf(x, mu = 0, sigma = 1):\n  return np.exp(-(x - mu) ** 2 * 0.5) * 1/(np.sqrt(2 * np.pi) * sigma)\n\nplt.clf()\nsns.histplot(x, stat = \"density\")\nplt.plot(y, normpdf(y), color = \"red\")\nplt.show()\n\n\n\n\nThe Generator object supports random sampling from many other distributions too. See the documenation for a complete list. The examples below are especially important.\n\n\n\n\n\n\n\nFunction\nDistribution\n\n\n\n\nintegers\nRandom integers from low (inclusive) to high (exclusive).\n\n\nchoice\nSample from an array with or without replacement.\n\n\nuniform\nUniformly distributed numbers.\n\n\nrandom\nUniformly distributed numbers on [0,1].\n\n\nnormal\nNormally distributed numbers.\n\n\nstandard_normal\nNormally distributed numbers with mean 0 and standard deviation 1.\n\n\nexponential\nExponentially distributed numbers with scale parameter.\n\n\nstandard_exponential\nExponentially distributed numbers with scale parameter 1.\n\n\n\n\n2.1.3.1 Quick exercise\nSimulate \\(10,000\\) random variables from a standard exponential. Verify that the random variables were simulated from an exponential by overlaying the standard exponential density.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nx = rng1.standard_exponential(10000)\ny = np.linspace(0, 6, 1000)\nplt.clf()\nsns.histplot(x, stat = \"density\")\nplt.plot(y, np.exp(-y), color = \"red\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n2.1.4 Simulating dice throws\nRandom number generators are often used to calculate probabilities that are hard to calculate by hand. You might be able to calculate, say, what he probability of getting a sum equal to \\(7\\) is when throwing \\(2\\) dice. But what about the probability that the maximal value \\(6\\) or that the sum is \\(7\\)? This is also doable by hand, but it’s easier to do it by machine.\n\nrng = np.random.default_rng(seed = 313)\nthrows = rng.integers(1, 7, size = (10000, 2))\n# throws contaisn 10000 rows of two dice throws.\ntotals = throws.sum(axis = 1)\nmaxs = throws.max(axis = 1)\n(totals, maxs)\n\n(array([6, 5, 6, ..., 9, 8, 9], dtype=int64),\n array([4, 3, 5, ..., 6, 5, 6], dtype=int64))\n\n\nTo calculate our probability, we need either totals == 7 or maxs == 6. We can use vectorized “OR” using the Numpy function logical_or to calculate this.\n\nx = np.logical_or(totals == 7, maxs == 6)\nx\n\narray([False, False, False, ...,  True, False,  True])\n\n\nThen we can take their mean to figure out the probability.\n\nx.mean()\n\n0.4134\n\n\nThus the probability is approximately \\(0.41\\). (Observe that Numpy automatically interprets True as 1 and False as 0 when forced to interpret boolean values as integers.)\nIn practice, we would write all of this in one go, probably using a function.\n\ndef prob(rng, n_reps = 10000):\n  throws = rng.integers(1, 7, size = (10000, 2))\n  return np.logical_or(throws.sum(axis = 1) == 7, throws.max(axis = 1) == 6).mean()\n  \nprob(rng)\n\n0.422\n\n\nThe result of this simulation is slightly different from the last one. This is due to randomnes, pure and simple. As we have already generated values from our rng, we would have to reset it to get the same value as before.\n\nrng = np.random.default_rng(seed = 313)\nprob(rng)\n\n0.4134\n\n\n\n\n2.1.5 More complex simulations\nSuppose that \\[X_1,X_2,\\ldots,X_k\\] are \\(k\\) iid exponential variables with density \\(\\frac{1}{\\beta} e^{-\\frac{1}{\\beta} x}, \\beta > 0\\), where \\(\\beta\\) is the scale parameter. It has been claimed that the minimum of \\(k\\) such variables are exponentially distributed with parameter \\(\\beta / k\\), i.e., \\(\\min(X_1,X_2,\\ldots,X_k)\\) has density \\(\\frac{k}{\\beta} e^{-\\frac{k}{\\beta} x}\\). Let’s try to verify this using simulations.\n\nrng = np.random.default_rng(seed = 313)\nx = rng.exponential(scale = 2, size = (10000, 10))\nminimas = x.min(axis = 1)\n\nLet’s plot and verify\n\nplt.clf()\nsns.histplot(minimas, stat = \"density\")\ny = np.linspace(0, 2, 100)\nplt.plot(y, 10/2 * np.exp(-10/2 * y), color = \"red\")\nplt.show()\nminimas\n\n\n\n\narray([0.01273357, 0.26827137, 0.0231306 , ..., 0.03719032, 0.08173748,\n       0.074854  ])\n\n\n\n2.1.5.1 Quick exercise\nTurn the code above into a function of \\(k\\) and \\(\\beta\\). Verify the formula visually for \\(k = 7, 99\\) and \\(\\beta = 1, 100\\).\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nrng = np.random.default_rng(seed = 313)\ndef plotter(rng, beta, k):\n  x = rng.exponential(scale = beta, size = (10000, k))\n  minimas = x.min(axis = 1)\n  sns.histplot(minimas, stat = \"density\")\n  y = np.linspace(0, minimas.max(), 100)\n  plt.plot(y, k/beta * np.exp(-k/beta * y), color = \"red\")\n  plt.show()\n\nplt.clf()\nplotter(rng, 1, 7)\nplt.clf()\nplotter(rng, 10, 7)\nplt.clf()\nplotter(rng, 100, 7)\nplt.clf()\nplotter(rng, 1, 99)\nplt.clf()\nplotter(rng, 10, 99)\nplt.clf()\nplotter(rng, 100, 99)"
  },
  {
    "objectID": "site/02-statistical-simulation.html#exercises",
    "href": "site/02-statistical-simulation.html#exercises",
    "title": "2  Statistical simulation",
    "section": "2.2 Exercises",
    "text": "2.2 Exercises\n\n2.2.1 Exercise 1\nWrite a function maximum_throw that approximates the probability of the maximal throw among throw throw. In other words, you throw throw dice, and calculate the maximum of them. To to approximate the probability, do this many times and use the np.unique function. (This is done in video 3!). Plot a bargraph of maximum_throw alongside a bar graph for minimum_throw when throw = 5 in both cases. What do you see?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndef maximum_throw(throws, rng, n_reps = 10 ** 5):\n    \"\"\" \n    Returns a dictionary where \"face\" is the face of the die and \"probability\"\n    is the approximate probability that said face is the maximal die after\n    \"throws\" throws of the dice.\n\n    n_reps is the number of repetitions used in the simulation.\n    \"\"\"\n    results = rng.integers(low = 1, high = 6 + 1, size = (n_reps, throws))\n    maximums = np.apply_along_axis(func1d = np.max, axis = 1, arr = results)\n    uniques, counts = np.unique(maximums, return_counts = True)\n    return {\"face\" : uniques, \"probability\" : counts / n_reps}\n\ndef minimum_throw(throws, rng, n_reps = 10 ** 5):\n    \"\"\" \n    Returns a dictionary where \"face\" is the face of the die and \"probability\"\n    is the approximate probability that said face is the minimal die after\n    \"throws\" throws of the dice.\n\n    n_reps is the number of repetitions used in the simulation; rng is a \n    random number generator.\n    \"\"\"\n    sims = rng.integers(low = 1, high = 6 + 1, size = (n_reps, throws))\n    results = np.apply_along_axis(func1d = np.min, axis = 1, arr = sims)\n    uniques, counts = np.unique(results, return_counts = True)\n    return {\"face\" : uniques, \"probability\" : counts / n_reps}\n\n# Let's simulate and plot!\nminimal_throws = minimum_throw(5, rng = rng)\nmaximal_throws = maximum_throw(5, rng = rng)\n\nfig, ax = plt.subplots(1,2)\nsns.barplot(x = \"face\", y = \"probability\", data = maximal_throws, ax = ax[0])\nsns.barplot(x = \"face\", y = \"probability\", data = minimal_throws, ax = ax[1])\nplt.show()\n\n\n\n\n\n\n\n\n\n2.2.2 Exercise 2\nWrite a function sum_throw that finds the probability of obtaining every possible sums of throw dice. Make a bar plot of the distribution when throw equals 7.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndef sum_throw(throws, rng, n_reps = 10 ** 5):\n    \"\"\" \n    Returns a dictionary where \"sum\" is the sum of the dice and \"probability\"\n    is the approximate probability that said sum occured.\n\n    n_reps is the number of repetitions used in the simulation.\n    \"\"\"\n    results = rng.integers(low = 1, high = 6 + 1, size = (n_reps, throws))\n    maximums = np.apply_along_axis(func1d = np.sum, axis = 1, arr = results)\n    uniques, counts = np.unique(maximums, return_counts = True)\n    return {\"sum\" : uniques, \"probability\" : counts / n_reps}\n\n# Let's simulate and plot!\nsimulated_throws = sum_throw(7, rng = rng)\nsns.barplot(\n    x = \"sum\", \n    y = \"probability\", \n    data = simulated_throws)\nplt.show()\n\n\n\n\n\n\n\n\n\n2.2.3 Exercise 3\nWrite a function that calculates the expected value of the sums in (2), i.e, sum(probs * value).\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndef sum_expected(throws, rng, n_reps = 10 ** 5):\n    \"\"\" \n    Approximates the expected value of sum of \"throws\" dice throws.\n\n    n_reps is the number of repetitions used in the simulation; rng is a \n    random number generator.\n    \"\"\"\n    simulated_throws = sum_throw(throws, rng)\n    sums = simulated_throws[\"sum\"]\n    probabilities = simulated_throws[\"probability\"]\n    return np.sum(sums * probabilities)\n\nLet’s plot the expected values, checking the linearity of expectation, i.e, \\(E(\\sum X_i) = \\sum E(X_i)\\). When every \\(X_i\\) have the same expectation, we’ll have \\(E(\\sum X_i ) = n E(X_1\\)) when we sum over \\(n\\) iterations.\n\nxs = range(1, 17)\nys = [sum_expected(throws, rng) for throws in xs]\nsns.scatterplot(x = xs, y = ys) # Expects the function y = a*x for some a!\nplt.show()\n\n\n\n\n\n\n\n\n\n2.2.4 Exercise 4\nWrite a function that finds the probability of obtaining every possible product of throw dice. Remember the docstring. Find the probability that the product of 5 dice throws exceeds 3888.\nHints: Look up np.max, np.sum.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndef product_throw(throws, rng, n_reps = 10 ** 5):\n    \"\"\" \n    Returns a dictionary where \"product\" is the product of \"throws\" throws and \n    \"probability\" is the approximate probability that the product equals \n    \"product\".\n\n    n_reps is the number of repetitions used in the simulation; rng is a \n    random number generator.\n    \"\"\"\n    sims = rng.integers(low = 1, high = 6 + 1, size = (n_reps, throws))\n    results = np.apply_along_axis(func1d = np.product, axis = 1, arr = sims)    \n    uniques, counts = np.unique(results, return_counts = True)\n    return {\"product\" : uniques, \"probability\" : counts / n_reps}\n\nsimulated_throws = product_throw(5, rng)\n# The probability that the product exceeds 3888:\nproducts = simulated_throws[\"product\"]\nprobabilities = simulated_throws[\"probability\"]\nsum([probabilities[i] for i, _ in enumerate(products) if products[i] > 3888])\n\n### Coda: Always a good idea to generalize your functions!\n\ndef throw_dice(throws, func, rng, n_reps = 10 ** 5):\n    \"\"\" \n    Returns a dictionary where \"result\" is the result of applying func to \n    \"throws\" throws and \"probability\" is the approximate probability that the \n    result equals \"result\".\n\n    n_reps is the number of repetitions used in the simulation.\n    \"\"\"\n    \n    simulation = rng.integers(low = 1, high = 7, size = (n_reps, throws))\n    result = np.apply_along_axis(func1d = func, axis = 1, arr = simulation)    \n    uniques, counts = np.unique(products, return_counts = True)\n    return {\"result\" : uniques, \"probability\" : counts / n_reps}\n\n\n\n\n\n\n2.2.5 Exercise 5\nMake a Python function that displays a histogram and density for the beta distribution, found in np.random. It takes the arguments n, a, and b, then simulates n observations from a beta distribution with parameters a and b. It displays a histogram of the observations on [0,1] with the true Beta density superimposed."
  },
  {
    "objectID": "site/03-limit-theorems.html#curriculum",
    "href": "site/03-limit-theorems.html#curriculum",
    "title": "3  Limit theorems",
    "section": "3.1 Curriculum",
    "text": "3.1 Curriculum\n\nDekking et al., Chapter 13: The law of large numbers\nDekking et al., Chapter 14: The central limit theorem\nThe short notes below."
  },
  {
    "objectID": "site/03-limit-theorems.html#notes",
    "href": "site/03-limit-theorems.html#notes",
    "title": "3  Limit theorems",
    "section": "3.2 Notes",
    "text": "3.2 Notes\nInformally speaking, the central limit theorem states that whenever \\(\\{X_i\\}_{i=0}^n\\) are independently and identically distributed according to some distribution \\(F\\) with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) (in mathematical notation \\(X_i \\stackrel{iid}{\\sim} F\\)) then \\(\\sqrt{n}\\frac{\\overline{X} - \\mu}{\\sigma}\\) is approximately normally distributed. (Here \\(\\overline{X}\\) denotes the sample mean.) There is a more precise statements of this result, but that is beyond the scope of this course; see e.g. wikipedia for more details.\nIt is fairly easy to illustrate the central limit theorem (CLT) using simulations.\n\nimport numpy as np\nrng = np.random.default_rng(seed=313)\n\nn = 100\nx = rng.gamma(3, 4, (n, 10000))\nmeans = x.mean(axis = 0)\nrescaled = np.sqrt(n) * (means - x.mean()) / np.std(x, ddof = 1)\n\nHere the rescaled, the rescaled sample means, approximates the distribution of \\(\\sqrt{n}\\frac{\\overline{X} - \\mu}{\\sigma}\\).\nLet’s simulate some true normal values to verify this:\n\nnormals = rng.standard_normal(10000)\n\nNow we can plot.\n\nimport seaborn as sns\nimport matplotlib.pylab as plt\nplt.clf()\nsns.histplot(rescaled, stat = \"density\")\nsns.histplot(normals, stat = \"density\")\nplt.show()\n\n\n\n\nObserve that the histograms overlap closely.\nQuick exericise: If you run the simulation with \\(n=10\\) instead of \\(n=100\\), do you think the histograms will match more or less closely? Run the simulation to verify.\nQuick exercise: Change the parameters of the gamma distribution rng.gamma; look up the documentation if you have to. Can you find parameters where the normal approximation in the central limit theorem is really poor when \\(n=100\\)?"
  },
  {
    "objectID": "site/03-limit-theorems.html#exercises",
    "href": "site/03-limit-theorems.html#exercises",
    "title": "3  Limit theorems",
    "section": "3.3 Exercises",
    "text": "3.3 Exercises\n\n3.3.1 Chapter 13\n\n13.1: Use Python to simulate from these variables. You can find all of these distributions in np.random, just initialize an rng object first! If you struggle finding the distributions, take a look at the Numpy documentation. Use the .mean method to approximate the probabilities and Wikipedia for the standard deviations (or use Numpy here as well.)\n13.2\n13.3\n13.5\n13.7: The function \\(F_n\\) is known a the empirical cumulative distribution function, shortened to ecdf. You can use Numpy to verify the claims of this exercise; write your own function or use the ecdf function from the statsmodels packae, from statsmodels.distributions.empirical_distribution import ECDF.\n13.9: If you are not able to find the required \\(a\\), find it using simulations.\n13.12: (Optional)\n\n\n\n3.3.2 Chapter 14\nIn some of these exercises you will need the quantile function for the normal distribution. You can find this in the scipy package, where it is called the ppf.\nHere is an example of \\(\\Phi^{-1}(0.95)\\).\n\nfrom scipy.stats import norm \nnorm.ppf(0.95, loc=0, scale=1)\n\n1.6448536269514722\n\n\n\n14.1: Solve this using Python.\n14.2: You can use np.random.beta to simulate from the Beta distribution. The parameters corresponding to the distribution in the exercise is \\(a=1\\), \\(b=4\\). Use this to compare the normal approximation to the true distribution of means. (Hint: Simulate a \\(625\\) variables a bunch of times and use the mean function. You can either calculate the variance of \\(X\\) yourself or use wikipedia.)\n14.8\n14.3\n14.4\n14.7\n14.9: This is an exercise about translating a textual problem into a mathematical one! It’s always a good idea to do all translations first and calculations afterward."
  },
  {
    "objectID": "site/04-exploratory-data-analysis.html#curriculum",
    "href": "site/04-exploratory-data-analysis.html#curriculum",
    "title": "4  Exploratory data analysis",
    "section": "4.1 Curriculum",
    "text": "4.1 Curriculum\n\nThe core readings of Dekking et al.\nPython notes about seaborn, Numpy, and scipy.\n\n\n4.1.1 Core readings\n\nDekking et al., Chapter 15: Exploratory data analysis: graphical summaries\nDekking et al., Chapter 16: Exploratory data analysis: numerical summaries\n\n\n\n4.1.2 Python\n\n4.1.2.1 Using seaborn for graphical summaries\n\n\n4.1.2.2 Numpy and scipy for numerical summaries"
  },
  {
    "objectID": "site/04-exploratory-data-analysis.html#exercises",
    "href": "site/04-exploratory-data-analysis.html#exercises",
    "title": "4  Exploratory data analysis",
    "section": "4.2 Exercises",
    "text": "4.2 Exercises"
  },
  {
    "objectID": "site/05-statistical-models.html#curriculum",
    "href": "site/05-statistical-models.html#curriculum",
    "title": "5  Basic statistical models and the bootstrap",
    "section": "5.1 Curriculum",
    "text": "5.1 Curriculum\n\nThe core readings of Dekking et al.\nPython notes about bootstrapping and modelling in Python.\n\n\n5.1.1 Core readings\n\nDekking et al., Chapter 17: Basic statistical models, except Chapter 17.4. Ignore the paragraphs about the Poisson model.\nDekking et al., Chapter 18: The bootstrap\n\n\n\n5.1.2 Basic statistical model in Python\n\n\n5.1.3 Bootstrap in Python"
  },
  {
    "objectID": "site/05-statistical-models.html#exercises",
    "href": "site/05-statistical-models.html#exercises",
    "title": "5  Basic statistical models and the bootstrap",
    "section": "5.2 Exercises",
    "text": "5.2 Exercises\n\n17.1\n17.2\n17.4 (use Python to visualize part b)\n17.5\n17.6\n17.10\n17.11\n18.1 (Hint: How many combinations are double counted due to the two 1s?)\n18.2 Use exact computations!\n18.3 Use exact computations!\n18.4 Use exact computations!\n18.6\n18.7\n18.12\n18.13"
  },
  {
    "objectID": "site/05-statistical-models.html#additional-resources",
    "href": "site/05-statistical-models.html#additional-resources",
    "title": "5  Basic statistical models and the bootstrap",
    "section": "5.3 Additional resources",
    "text": "5.3 Additional resources"
  },
  {
    "objectID": "site/06-unbiased-estimators-efficiency.html#curriculum",
    "href": "site/06-unbiased-estimators-efficiency.html#curriculum",
    "title": "6  Unbiased estimators and efficiency",
    "section": "6.1 Curriculum",
    "text": "6.1 Curriculum\n\n6.1.1 Core readings\n\nDekking et al., Chapter 19: Unbiased estimators\nDekking et al., Chapter 20: Efficiency and mean squared error"
  },
  {
    "objectID": "site/06-unbiased-estimators-efficiency.html#exercises",
    "href": "site/06-unbiased-estimators-efficiency.html#exercises",
    "title": "6  Unbiased estimators and efficiency",
    "section": "6.2 Exercises",
    "text": "6.2 Exercises"
  },
  {
    "objectID": "site/07-constructing-estimators.html#curriculum",
    "href": "site/07-constructing-estimators.html#curriculum",
    "title": "7  Constructing estimators",
    "section": "7.1 Curriculum",
    "text": "7.1 Curriculum\n\n7.1.1 Core readings\n\nDekking et al., Chapter 21: Maximum likelihood\nDekking et al., Chapter 22: The method of least squares"
  },
  {
    "objectID": "site/07-constructing-estimators.html#exercises",
    "href": "site/07-constructing-estimators.html#exercises",
    "title": "7  Constructing estimators",
    "section": "7.2 Exercises",
    "text": "7.2 Exercises\n\n7.2.0.1 Maximum likelihood.\n\n21.4 (Poisson ML and invariance principle)\n21.6 (Modified uniform distribution ish)\n21.7 (Rayleigh distribution)\n21.8 (Multinomial)\n21.10 (Pareto)\n21.14 (Uniform)\n21.15 (Two parameters: Copy into Python and fix.)\nUnfinished: Log-normal problem\nUnfinished: Uniform on both sides, \\(\\log(u - l)\\).\n\n\n\n7.2.0.2 Least squares.\n\n22.1 (Drawing linear regression.)\n22.2 (Leverage point.)\n22.7 (Exponential?)\n22.8 (Conceptual).\n22.9 (Without intercept.)\n22.12 (Unbiased.)"
  },
  {
    "objectID": "site/07-constructing-estimators.html#additional-resources",
    "href": "site/07-constructing-estimators.html#additional-resources",
    "title": "7  Constructing estimators",
    "section": "7.3 Additional resources",
    "text": "7.3 Additional resources"
  },
  {
    "objectID": "site/08-confidence-intervals.html#curriculum",
    "href": "site/08-confidence-intervals.html#curriculum",
    "title": "8  Confidence intervals",
    "section": "8.1 Curriculum",
    "text": "8.1 Curriculum\n\n8.1.1 Core readings\nDekking et al., Chapter 23: Confidence intervals for the mean\nDekking et al., Chapter 24: More on confidence intervals\n\n8.1.1.1 Scipy functions\n\nhttps://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html\nhttps://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.zscore.html\nhttps://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_rel.html\nhttps://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html"
  },
  {
    "objectID": "site/08-confidence-intervals.html#exercises",
    "href": "site/08-confidence-intervals.html#exercises",
    "title": "8  Confidence intervals",
    "section": "8.2 Exercises",
    "text": "8.2 Exercises\n\n8.2.1 Theory exercises\nDekking et al., Chapter 23: 23.1, 23.2, 23.3, 23.4, 23.6, 23.11, 23.12\n\n\n8.2.2 Python exercises\nSolve the following exercises using Python. In both exercises you are asked to construct a bootstrap interval using the provided data, but you should use the t-test too. Dekking et al., Chapter 23: 23.5(a,b), 23.9\nA1: Make your own custom function tinterval(data, alpha) that constructs a confidence interval for the mean with confidence level \\(1-\\alpha\\). Assume the data are normally distributed. A2: Make your own custom function bootinterval(data, alpha, rng) that constructs are bootstrap percentile confidence interval with confidence level \\(1-\\alpha\\) based on the random number generator \\(rng\\).\n\n\n8.2.3 Simulation exercises"
  },
  {
    "objectID": "site/08-confidence-intervals.html#additional-resources",
    "href": "site/08-confidence-intervals.html#additional-resources",
    "title": "8  Confidence intervals",
    "section": "8.3 Additional resources",
    "text": "8.3 Additional resources\nDekking et al., Chapter 24 covers confidence intervals for different quantities than the mean. This chapter is not part of the curriculum but is still worth reading."
  },
  {
    "objectID": "site/09-testing-hypotheses.html#curriculum",
    "href": "site/09-testing-hypotheses.html#curriculum",
    "title": "9  Testing hypotheses",
    "section": "9.1 Curriculum",
    "text": "9.1 Curriculum\n\nDekking et al., Chapter 25: Testing hypotheses: essentials\nDekking et al., Chapter 26: Testing hypotheses: elaboration ## Exercises"
  },
  {
    "objectID": "site/09-testing-hypotheses.html#additional-resources",
    "href": "site/09-testing-hypotheses.html#additional-resources",
    "title": "9  Testing hypotheses",
    "section": "9.2 Additional resources",
    "text": "9.2 Additional resources"
  },
  {
    "objectID": "site/10-testing-the-sample-mean.html#curriculum",
    "href": "site/10-testing-the-sample-mean.html#curriculum",
    "title": "10  Testing the sample mean",
    "section": "10.1 Curriculum",
    "text": "10.1 Curriculum\n\nDekking et al., Chapter 27: The t-test\nDekking et al., Chapter 28: Comparing two samples\nThe following article on AB testing."
  },
  {
    "objectID": "site/10-testing-the-sample-mean.html#exercises",
    "href": "site/10-testing-the-sample-mean.html#exercises",
    "title": "10  Testing the sample mean",
    "section": "10.2 Exercises",
    "text": "10.2 Exercises"
  },
  {
    "objectID": "site/10-testing-the-sample-mean.html#recommended-resources",
    "href": "site/10-testing-the-sample-mean.html#recommended-resources",
    "title": "10  Testing the sample mean",
    "section": "10.3 Recommended resources",
    "text": "10.3 Recommended resources"
  },
  {
    "objectID": "site/11-statistical-learning.html#curriculum",
    "href": "site/11-statistical-learning.html#curriculum",
    "title": "11  Statistical learning, machine learning, and statistics",
    "section": "11.1 Curriculum",
    "text": "11.1 Curriculum\n\nJames et al., Chapter 1: Introduction\nJames et al., Chapter 2: Statistical learning"
  },
  {
    "objectID": "site/11-statistical-learning.html#exercises",
    "href": "site/11-statistical-learning.html#exercises",
    "title": "11  Statistical learning, machine learning, and statistics",
    "section": "11.2 Exercises",
    "text": "11.2 Exercises\nExercises from James et. al. Chapter 2. ### Conceptual - Roughly reproduce 2.3 Introduction to R in Python. - 2.4.1. - 2.4.5. - 2.4.2. - 2.4.3. Do it on pen and paper; use the internet if needed - 2.4.4.\n\n11.2.1 Applied\n\n2.4.8 (do it in Python instead; you should know the functions already!)\n2.4.9 (highly exam relevant!)"
  },
  {
    "objectID": "site/11-statistical-learning.html#recommended-resources",
    "href": "site/11-statistical-learning.html#recommended-resources",
    "title": "11  Statistical learning, machine learning, and statistics",
    "section": "11.3 Recommended resources",
    "text": "11.3 Recommended resources"
  },
  {
    "objectID": "site/12-basics-of-linear-regression.html#curriculum",
    "href": "site/12-basics-of-linear-regression.html#curriculum",
    "title": "12  Basics of multiple linear regression",
    "section": "12.1 Curriculum",
    "text": "12.1 Curriculum\n\nJames et al., Chapter 3: Statistical learning"
  },
  {
    "objectID": "site/12-basics-of-linear-regression.html#exercises",
    "href": "site/12-basics-of-linear-regression.html#exercises",
    "title": "12  Basics of multiple linear regression",
    "section": "12.2 Exercises",
    "text": "12.2 Exercises\nAll exercises in Chapter 3 are recommended. #### Conceptual - 3.7.1 - 3.7.2 - 3.7.3 - 3.7.4 Verify your answers using a suitable Python simulation. - 3.7.5 - 3.7.6 Verify it graphically in Python using a suitable simulation.\n\n12.2.0.1 Applied\n\n3.7.8\n3.7.9-3.7.12"
  },
  {
    "objectID": "site/12-basics-of-linear-regression.html#recommended-resources",
    "href": "site/12-basics-of-linear-regression.html#recommended-resources",
    "title": "12  Basics of multiple linear regression",
    "section": "12.3 Recommended resources",
    "text": "12.3 Recommended resources"
  },
  {
    "objectID": "site/13-inference-for-linear-regression.html#curriculum",
    "href": "site/13-inference-for-linear-regression.html#curriculum",
    "title": "13  Inference for multiple linear regression",
    "section": "13.1 Curriculum",
    "text": "13.1 Curriculum"
  },
  {
    "objectID": "site/13-inference-for-linear-regression.html#exercises",
    "href": "site/13-inference-for-linear-regression.html#exercises",
    "title": "13  Inference for multiple linear regression",
    "section": "13.2 Exercises",
    "text": "13.2 Exercises"
  },
  {
    "objectID": "site/13-inference-for-linear-regression.html#recommended-resources",
    "href": "site/13-inference-for-linear-regression.html#recommended-resources",
    "title": "13  Inference for multiple linear regression",
    "section": "13.3 Recommended resources",
    "text": "13.3 Recommended resources"
  }
]