---
title: "EBA35002 Fall 2022"
subtitle: Written exam
author: Jonas Moss
format:
  html:
    embed-resources: true
    colorlinks: true
editor: visual
---

All sub-exercises are equally weighted.

## 1 Mathematical questions

We have $n$ independent observations $X_1,X_2,\ldots,X_n$ from the logistic distribution $$
f(x;\mu,\sigma)=\frac{1}{x\sigma\sqrt{2\pi}}\exp\left(-\frac{(\ln x-\mu)^{2}}{2\sigma^{2}}\right),\quad x\in(-\infty,\infty).
$$

Use the following properties freely in the following exercises.

1.  $X$ is log-normally distributed with parameters $\mu,\sigma$ if and only if $\log X$ is normally distributed with parameters mean $\mu$ and variance $\sigma^{2}$.
2.  The mean of a log-normal $X$ is $EX=e^{\mu+\frac{\sigma^{2}}{2}}$ and its variance is $(e^{\sigma^{2}}-1)e^{2\mu+\sigma^{2}}$.
3.  $\hat{\mu} = \overline{\log X}$ is the maximum likelihood estimator of $\mu$ and the biased sample variance $\widehat{\sigma^2} = \sum_{i=1}^{n}(\log X_{i}-\overline{\log X})^{2}/n$ is the maximum likelihood estimator of $\sigma^{2}$.
4.  The Fisher information matrix of $(\mu,\sigma^{2})$ is $$
    I(\mu,\sigma^{2})=\left[\begin{array}{cc}
    \frac{1}{\sigma^{2}} & 0\\
    0 & \frac{1}{2\sigma^{4}}
    \end{array}\right].
    $$

### 1.a.

There are two ways to show that $\sqrt{n}(\overline{\log X}-\mu)$ converges in distribution to $N(0,\sigma^2)$. What are these two?

::: {.callout-note collapse="true"}
#### Solution

By the central limit theorem and property (2) above, the asymptotic distribution of $\sqrt{n}(\overline{\log X}-\mu)$ is normal with standard deviation $\sigma$. (b) You can also use the fact that the maximum likelihood estimator converges in distribution to a normal distribution with covariance matrix $I^{-1}$.
:::

### 1.b.

What is the asymptotic distribution of $\sqrt{n}(\overline{X}-E(X_{1}))$?

::: {.callout-note collapse="true"}
#### Solution

By the central limit theorem and property (3) above, the asymptotic distribution of $\sqrt{n}(\overline{X}-\mu)$ is normal with standard deviation $\sqrt{(e^{\sigma^{2}}-1)e^{2\mu+\sigma^{2}}}$.
:::

### 1.c.

One can show that the median of the log-normal distribution is $e^{\mu}$. What is the maximum likelihood estimator of $e^{\mu}$?

::: {.callout-note collapse="true"}
#### Solution

By the invariance principle, the maximum likelihood estimator of $e^\mu$ is $e^{\hat{\mu}},$ where $\hat{\mu}$ is the maximum likelihood estimator of $\mu$.
:::

### 1.d

What is the asymptotic distribution of $\sqrt{n}(e^{\overline{\log X}}-e^{\mu})$?

::: {.callout-note collapse="true"}
#### Solution

By the delta method, the asymptotic distribution is normal with standard deviation $|g'(\mu)|\sigma$. Now $g(\mu) = e^\mu$ has derivative $g'(\mu) = e^\mu$, hence the standard deviation is $e^\mu\sigma$.
:::

### 1.e.

What is the maximum likelihood estimator of the mean of $X_{1}$?

::: {.callout-note collapse="true"}
#### Solution

Using the invariance principle and point (3), the maximum likelihood estimator is $e^{\hat{\mu} + \widehat{\sigma^2}/2}$
:::

### 1.f.

Let $g(x,y)=e^{x+\frac{y}{2}}$. Calculate the partial derivatives $$
\frac{\partial g}{\partial x},\quad\frac{\partial g}{\partial y}.
$$

::: {.callout-note collapse="true"}
#### Solution

The partial derivatives are $e^{x+y/2}$ and $\frac{1}{2}e^{x+y/2}$.
:::

### 1.g.

Let $\theta$ be a $k$-dimensional vector parameter and $g: \mathbb{R}^k \to \mathbb{R}$ be a continuously differentiable function. Moreover, suppose $\sqrt{n}(\hat{\theta} -\theta)\stackrel{d}{\to} N(0,\Sigma)$. What is the asymptotic distribution of $\sqrt{n}(g(\hat{\theta})-g(\theta))$? What is the name of this result?

::: {.callout-note collapse="true"}
#### Solution

The *delta method (rule)* states that $\sqrt{n}(g(\hat{\theta})-g(\theta))$ converges to a normal with variance $\nabla g(\theta)^T \Sigma \nabla g(\theta)$, where $\nabla g(\theta)$ is the gradient of $g$ at $\theta$.
:::

### 1.h.

Show that the asymptotic distribution of the maximum likelihood estimator in 1.e., i.e., $\sqrt{n}(\hat{\theta}_{ML}-\theta)$, where $\theta = e^{\mu + \sigma^2/2}$, has variance $\sigma^2(1+\frac{1}{2}\sigma^2) e^{2\mu + \sigma^2}$.

::: {.callout-note collapse="true"}
#### Solution

We use the delta method $g^T\Sigma g$, where $\Sigma$ is the inverse Fisher information. Then, putting $\theta = e^{\mu + \sigma^2/2}$, \begin{eqnarray*}
\left[\begin{array}{c}
\theta\\
\frac{1}{2}\theta
\end{array}\right]^{T}\left[\begin{array}{cc}
\sigma^{2} & 0\\
0 & 2\sigma^{4}
\end{array}\right]\left[\begin{array}{c}
\theta\\
\frac{1}{2}\theta
\end{array}\right] & = & \left[\begin{array}{c}
\theta\\
\frac{1}{2}\theta
\end{array}\right]^{T}\left[\begin{array}{c}
\theta\sigma^{2}\\
\theta\sigma^{4}
\end{array}\right]\\
 & = & \theta^{2}\sigma^{2}+\frac{1}{2}\theta^{2}\sigma^{4}.
\end{eqnarray*} Since $\theta^2 = e^{2\mu + \sigma^2}$ the desired expression is true.
:::

### 1.i.

Now we have two estimators of $EX_{1}$. Which one should you prefer? You may use that $e^{x^2} > \frac{1}{2} x^4 - x^2 + 1$ whenever $x\neq 0$.

::: {.callout-note collapse="true"}
#### Solution

(a) You may argue that maximum likelihood estimators are efficient, hence you prefer the maximum likelihood estimator.
(b) You may use the fact that $e^{x^2} > \frac{1}{2} x^4 - x^2 + 1$ whenever $x\neq 0$ and that the asymptotic variances of the two methods are $$\ensuremath{\sigma^{2}(1+\frac{1}{2}\sigma^{2})e^{2\mu+\sigma^{2}}}<(e^{\sigma^{2}}-1)e^{2\mu+\sigma^{2}},$$ now divide by $e^{2\mu+\sigma^{2}}$ on both sides to get $$ \ensuremath{\sigma^{2}(1+\frac{1}{2}\sigma^{2})}<(e^{\sigma^{2}}-1),$$ when we add $1$ to both sides, we obtain $$\ensuremath{\sigma^{2}+\frac{1}{2}\sigma^{4}}+1<e^{\sigma^{2}}.$$ Thus the maximum likelihood estimator has the smallest variance for any $\sigma^2>0$.
:::

### 1.j

Construct an approximate $95\%$ confidence interval for the expectation $E(X_1)$ using the maximum likelihood method.

::: {.callout-note collapse="true"}
#### Solution

Suppose that $\sqrt{n}(\hat{\theta} - \theta) \to N(0, \tau)$. Using the $Z$-interval construction, a $(1-\alpha)\%$ confidence interval for $\theta$ is $$CI(\theta; 1-\alpha) = [\hat{\theta} - \Phi^{-1}(1-\alpha/2)\sqrt{\tau}/\sqrt{n}, \hat{\theta} + \Phi^{-1}(1-\alpha/2)\sqrt{\tau}/\sqrt{n}],$$ where $\Phi^{-1}$ is the quantile function of the normal distribution, called ppf in Scipy. Recall that $$\Phi^{-1}(1-\alpha/2)\approx 1.96$$ when $\alpha = 0.95$

Thus an approximate confidence interval is $$CI(\theta; 1-\alpha) = [\hat{\theta} - \Phi^{-1}(1-\alpha/2)\sqrt{\tau}/\sqrt{n}, \hat{\theta} + \Phi^{-1}(1-\alpha/2)\sqrt{\tau}/\sqrt{n}],$$ Now plug in $\hat{\theta} = e^{\hat{\mu} + 0.5\widehat{\sigma^2}}$ and $\hat{\tau} = \sigma^2(1+\frac{1}{2}\sigma^2) e^{2\mu + \sigma^2}$.
:::

### 1.k

Using the results in the previous exercise, construct a confidence interval for y when $\overline{\log X} = 1$ and $\overline{\log X^2} = 2$.

::: {.callout-note collapse="true"}
#### Solution

We use that the unbiased sample variance can be written as \begin{eqnarray*}
\widehat{\sigma^{2}}=\sum_{i=1}^{n}(Y_{i}-\overline{Y})^{2}/n & = & \overline{Y^{2}}-\overline{Y}^{2}\\
 & = & 2-1\\
 & = & 1
\end{eqnarray*} And then $$
\hat{\mu}=\overline{Y}=1
$$ \begin{eqnarray*}
e^{\hat{\mu}+\frac{1}{2}\widehat{\sigma^{2}}} & = & e^{\hat{\mu}+\frac{1}{2}\widehat{\sigma^{2}}}\\
 & = & e^{1+\frac{1}{2}1}\\
 & = & e^{3/2}.
\end{eqnarray*} Moreover, $${\sqrt{\hat{\sigma}^{2}(1+\frac{1}{2}\hat{\sigma}^{2})e^{2\hat{\mu}+\hat{\sigma}^{2}}}}=\sqrt{\frac{3}{2}e^{3}}=\sqrt{\frac{3}{2}}e^{3/2}.$$ Plug this into the confidence interval to obtain $$e^{3/2}\pm e^{3/2}\sqrt{\frac{3}{2}}\cdot1.96/\sqrt{n}.$$
:::

## 2 Regression questions

### 2.a

Consider the following code:

```{python}
import statsmodels.api as sm
import statsmodels.formula.api as smf
import pandas as pd
import numpy as np
weather = pd.read_csv("weatherAUS.csv")
weather.RainToday = 1 * (weather.RainToday == "Yes")
weather.RainTomorrow = 1 * (weather.RainTomorrow == "Yes")
model = smf.logit("RainTomorrow ~ Location + RainToday", data = weather).fit(disp=0)

print(f"weather.Location.dtype = {weather.Location.dtype}")
print(f"model.llf = {model.llf}")
print(f"model.llnull = {model.llnull}")
```

1.  Alice wants to use the model `"RainTomorrow ~ C(Location) + RainToday"` instead of `"RainTomorrow ~ Location + RainToday"`. Is this a good idea?
2.  Use `llf` and `llnull` to reproduce McFadden's $R^2$. (I.e., what you get from `model.prsquared`).

::: {.callout-note collapse="true"}
#### Solution

1.  Doesn't matter what you do, since `Location` is categorical.
2.  You must write `1 - model.llf/model.llnull`.
:::

### 2.b

Suppose you have $3$ categorical covariates `a,b,c` with $14$, $3$ and $9$ levels each. How many regression coefficients are there in the model `y~a+b*c`? How about `y ~ a*b*c`?

::: {.callout-note collapse="true"}
#### Solution

1.  There are $14 + 3\cdot 9 = 41$ coefficients all in all, including the intercept.
2.  There are $14\cdot3\cdot9 = 378$ coefficients all in all, including the intercept.
:::

### 2.c

Consider the following code:

```{python}
import statsmodels.api as sm
import statsmodels.formula.api as smf
import numpy as np
cpssw8 = sm.datasets.get_rdataset("CPSSW8", "AER").data
model = smf.ols("np.log(earnings) ~ age + region", data = cpssw8).fit()
print(model.params)
print(set(cpssw8.region))
params = np.array(model.params)
print(params)
```

1.  What would have been the value of `region[T.Midwest]` in the regression model `np.log(earnings) ~ age + region - 1`?
2.  Write a Python function of `params` to predict the log-earnings of a 40 year old living in the South,

::: {.callout-note collapse="true"}
#### Solution

1.  It equals the intercept of the current model.
2.  You would write a function that returns `params[0] + params[2] + 40 * params[4]`.
:::

### 2.d

Bob's in big trouble! His boss Alice wants him to do a linear regression `lwage ~ age + gender`, where `lwage` is a centered measure of wages in Alice's company. But Bob has somehow managed to throw away her `lwage` data, replacing it with the variable `dwage = 1 * (lwage >= 0)` instead! What should Bob do to fulfill her boss's wish, and why would it work?

::: {.callout-note collapse="true"}
#### Solution

Bob should use Probit regression `dwage ~  age + gender`. It works since, if the residual error is normal, the probit model is equivalent to the model `lwage ~ age + gender` in our current setting.
:::
