# Statistical learning, machine learning, and statistics

::: callout-note
**Update:** 25/10: More about `R`.

:::

## Curriculum

* James et al., Chapter 1: Introduction
* James et al., Chapter 2: Statistical learning

## Notes
The book by James et al. is [here](https://trevorhastie.github.io/ISLR/). It uses `R` instead of Python, but we have chosen to use it since it is very popular and well-received. You don't need to produce any `R` yourself in this course, but you should be able to read and understand the simple code in the book. Don't be scared by the `R` code! 

One of the exercises this time is to roughly reproduce the book's introduction to `R`, given in Section 2.3.

For instance, the book writes

```{r}
# R code
c(1, 2, 3, 5)
```

Here `c` is the concatenation operator, which combines several elements into a vector. It is roughly equivalent to `np.array([1,2,3,5])`. Other examples are:

* `rnorm` is equivalent to `rng.normal`,
* `set.seed` is equivalent to `np.random_default_rng`,
* `dev.off` is equivalent to `plt.clf`.

In later chapters the book will use Python functions we haven't covered yet. I will introduce the Python equivalents when they show up.

## Exercises
Exercises from James et. al. Chapter 2.

### Applied
- Roughly reproduce 2.3 Introduction to `R` in Python: The main purpose of this exercise is to gain confidence in reading `R`, you will never be asked to write `R` in thi course.
- 2.4.8 (do it in Python instead; you should know the functions already!) Link to the book website can be found above.
- 2.4.9 This is highly exam relevant. Some points: (a) The equivalent of `range` in Python is `range = lambda x: (np.min(x), np.max(x))`. We'll go through this exercise in the exercise session.

### Conceptual
- 2.4.1.
- 2.4.5.
- 2.4.2.
- 2.4.3. Do it on pen and paper; use the internet if needed
- 2.4.4.
