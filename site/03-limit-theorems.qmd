# Limit theorems

::: {.callout-note}
**Updated 5/9:** Added short note about law of large numbers.
:::

## Curriculum
1.  Dekking et al., Chapter 13: The law of large numbers

2.  Dekking et al., Chapter 14: The central limit theorem

3. The short notes below.

## Notes

### Law of large numbers
The law of large numbers states that the sample means $\overline{X}_i=\frac{1}{n}\sum_{i=1}^n X_i$ converges to the population mean 
$EX$ in probability as $n\to \infty$. We illustrate this result using the 
continuous uniform distribution on $[0,1]$, called `rng.uniform` in Numpy.
The density of this distribution is $f(x)=1[x\in[0,1]]$, where $1(x\in A)$ denotes the indicator function you are used to from the previous probability course. (It is commonly written as $I[x\inA]$ or just $I(A)$.) Its expectation is equal to 
$1/2$.

#### Quick exercise
Show that the expected value of the uniform distribution on $[a,b]$ is $(b+a)/2$ using the definition $f(x;a,b) = 1/(b-a)1[x\in[a,b]]$. 
::: {.callout-tip collapse="true"}
#### Solution
$$\int_a^b x/(b-a)dx = \frac{1}{2}(b^2-a^2)/(b-a) = \frac{1}{2}(b+a)(b-a)/(b-a) = \frac{1}{2}(b+a)$$.
:::

We first simulate $n=10000$ of uniforms and store them `x`. These correspond to a sequence $X_1,X_2,X_3,\ldots X_n$ of independently and identically distributed
uniform variables. 
```{python}
import numpy as np
import seaborn as sns
import matplotlib.pylab as plt

rng = np.random.default_rng(seed=313)
x = rng.uniform(0, 1, 10000)
```

Now we compute the partial means $\overline{X}_j = \frac{1}{n}\sum_{i=1}^n X_i$. This is most conveniently done using the `np.cumsum` function, which calculate the cumulative sums of `x`, i.e., `y[i] = x[0:(i+1)].sum()` when `y=np.cumsum(x)`.

```{python}
means = np.cumsum(x) / (np.arange(len(x)) + 1)
```

Now we can illustrate the law of large numbers:
```{python}
plt.plot(means)
plt.axhline(y = 0.5)
plt.show()
```

Observe that the partial means get arbitrarily close to the true expected value ($0.5$) as $n$ increases. 

#### Quick exercise
Make a function `sim_lln(n, gen)` that takes a number `n` and a random number generator such as `lambda x: rng.uniform(2, 3, x)`, simulates `n` numbers, and makes a plot similar to the one above.

### Central limit theorem
Informally speaking, the central limit theorem states that whenever $\{X_i\}_{i=0}^n$ are independently and identically distributed according to some distribution $F$ with mean $\mu$ and standard deviation $\sigma$ (in mathematical notation $X_i \stackrel{iid}{\sim} F$) then $\sqrt{n}\frac{\overline{X} - \mu}{\sigma}$ is approximately normally distributed. (Here $\overline{X}$ denotes the sample mean.) There is a more precise statements of this result, but that is beyond the scope of this course; see e.g. wikipedia for more details.

It is fairly easy to illustrate the central limit theorem (CLT) using simulations.

```{python}
import numpy as np
rng = np.random.default_rng(seed=313)

n = 100
x = rng.gamma(3, 4, (n, 10000))
means = x.mean(axis = 0)
rescaled = np.sqrt(n) * (means - x.mean()) / np.std(x, ddof = 1)
```

Here the `rescaled`, the rescaled sample means, approximates the distribution of $\sqrt{n}\frac{\overline{X} - \mu}{\sigma}$.

Let's simulate some true normal values to verify this:

```{python}
normals = rng.standard_normal(10000)
```

Now we can plot.
```{python}
import seaborn as sns
import matplotlib.pylab as plt
plt.clf()
sns.histplot(rescaled, stat = "density")
sns.histplot(normals, stat = "density")
plt.show()
```

Observe that the histograms overlap closely. 

**Quick exericise:** If you run the simulation with $n=10$ instead of $n=100$, do
you think the histograms will match more or less closely? Run the simulation to
verify. 

**Quick exercise:** Change the parameters of the gamma distribution `rng.gamma`;
look up the documentation if you have to. Can you find parameters where the
normal approximation in the central limit theorem is really poor when $n=100$?

## Exercises
### Chapter 13
* 13.1: Use Python to simulate from these variables. You can find all of these
distributions in `np.random`, just initialize an `rng` object first! If you
struggle finding the distributions, take a look at the Numpy documentation. Use
the `.mean` method to approximate the probabilities and Wikipedia for the standard
deviations (or use Numpy here as well.)
* 13.2
* 13.3
* 13.5
* 13.7: The function $F_n$ is known a the empirical cumulative distribution 
function, shortened to `ecdf`. You can use Numpy to verify the claims of this
exercise; write your own function or use the `ecdf` function from the
statsmodels packae, `from statsmodels.distributions.empirical_distribution import ECDF`.
* 13.9: If you are not able to find the required $a$, find it using simulations.
* 13.12: (Optional)

### Chapter 14
In some of these exercises you will need the quantile function for the normal
distribution. You can find this in the `scipy` package, where it is called the
`ppf`.

Here is an example of $\Phi^{-1}(0.95)$.
```{python}
from scipy.stats import norm 
norm.ppf(0.95, loc=0, scale=1)
```

* 14.1: Solve this using Python.
* 14.2: You can use `np.random.beta` to simulate from the Beta distribution. The
parameters corresponding to the distribution in the exercise is $a=1$, $b=4$.
Use this to compare the normal approximation to the true distribution of means.
(*Hint:* Simulate a $625$ variables a bunch of times and use the mean function. 
You can either calculate the variance of $X$ yourself or use wikipedia.)
* 14.8
* 14.3
* 14.4
* 14.7
* 14.9: This is an exercise about translating a textual problem into a mathematical one! It's always a good idea to do all translations first and calculations afterward.
