{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EBA3500-exercises4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gBmH_OxWrtu"
      },
      "source": [
        "# EBA3500 Exercises 4: Classification, proper scoring rules, and likelihood\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKGTW6jQZ-ZW"
      },
      "source": [
        "# Exercise 1: Minimize and maximize\n",
        "\n",
        "### (a) Maximum likelihood\n",
        "Let $y_1, ..., y_n$ be independent $0-1$ variables with success probability $p$. Use differentiation to show that the maximum likelihood estimator of $p$ is $\\hat p = \\overline{y}$, the mean of $y_1, ..., y_n$. (*Hint*: Find the expression for the likelihood in the lecture notes. Google \"differentiation maximization\" or something if you have forgotten how to optimize using differentiation.)\n",
        "\n",
        "### (b) Least squares\n",
        "Let $y_1, ..., y_n$ be independent variables and $\\overline{y}$ be the mean of $y_1, ..., y_n$. Using differentiation, show that $\\overline{y}$ minimizes $$m \\mapsto \\sum_{i=1}^{n}(y_{i}-m)^{2}.$$\n",
        "In other words, $\\sum_{i=1}^{n}(y_{i}-m)^{2}\\leq \\sum_{i=1}^{n}(y_{i}-\\overline {y})^{2}$, no matter what $m$ is.\n",
        "\n",
        "### (c) Log score\n",
        "Let $y_1, ..., y_n$ be independent $0-1$ variables with success probability $p$. Show that $\\overline{y}$ minimizes the sum of log scores\n",
        "$$m \\mapsto \\sum_{i=1}^{n}-y_{i}\\log m-(1-y_{i})\\log(1-m).$$ (*Hint:* Does this exercise have any relationsship to exercise (a)?)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lP-u-L3QbUp_"
      },
      "source": [
        "\n",
        "## Exercise 2: Using the logistic regression\n",
        "[This page](https://www.geo.fu-berlin.de/en/v/soga/Basics-of-statistics/Logistic-Regression/Logistic-Regression-in-R---An-Example/index.html) works with logistic regression on a particular dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3z0irZ-aZ4le"
      },
      "source": [
        "\n",
        "### (a) Download data\n",
        "\n",
        "Load the data set at https://userpage.fu-berlin.de/soga/200/2010_data_sets/hurricanes.xlsx into a data frame `hurricanes`. Make a `sns.pairplot` and look at the correlation matrix. For more information about the data, look at the link. Some of the correlations are extremely high. Why? (*Hint*: Look at the lecture notes. Be sure to remove columns\n",
        "that aren't numeric using [`drop`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html). You can use `df.corr()` to calculate the correlation matrix.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsOcBirhc4ns"
      },
      "source": [
        "### (b) Correlation plot\n",
        "The correlation matrix is hard to read. Modify the code from [here](https://seaborn.pydata.org/examples/many_pairwise_correlations.html) or [here](https://medium.com/@szabo.bibor/how-to-create-a-seaborn-correlation-heatmap-in-python-834c0686b88e) to make it readable. (*Hint:* Start out with `sns.heatmap(dataframe.corr())`.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVKp-7QFe40d"
      },
      "source": [
        "### (c) Fitting a logistic regression\n",
        "Make a new column in `hurricanes` called `Type.new`. A value in this column equals $0$ if `Type == 0` and $1$ otherwise. Use `sns.lmplot` to plot a logistic regression `\"Type_new ~ FirstLat\"`. (*Hint:* First make a column `c` that is $1$ if `Type == 0` and $0$ otherwise. Then modify it to be $0$ if `Type == 0` and $1$ otherwise using `1 - c`, or `(1 - 1 * (hurricanes[\"Type\"] == 0))`.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgVRTrA7gPTm"
      },
      "source": [
        "### (d) Finding the $R^2$\n",
        "Fit a logistic and a probit regression model to the data, and report the McFadden $R^2$ values. Are these high or low?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPOksnnkiN7M"
      },
      "source": [
        "### (e) Finding other predictors\n",
        "Using the `Type.new` variable as a response, find other reasonable predictors and plot them. (*Hint:* Can you use `hurricanes.corr()` for this?)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDcDiLUnjMaO"
      },
      "source": [
        "## Exercise 3: McFadden's $R^2$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgAyoqLBjS-8"
      },
      "source": [
        "### (a) The McFadden's correlation\n",
        "The least squares $R^2$ can be modified into a correlation by multiplication with a sign and taking roots. Propose a similar McFadden correlation for binary regression and make a program that calculates it for logistic regression. (*Hint:* The link functions are increasing, hence $F(a+bx)$ is increasing if $b>0$ and decreasing if $b < 0$.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKae6AsUj96U"
      },
      "source": [
        "### (b) The McFadden correlation matrix\n",
        "Assume the McFadden correlation between two vectors `x` and `y` is calculated by `mcf_corr(x,y)`. What does the following function do? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wuq_kXwJooKd"
      },
      "source": [
        "def mystery(df):\n",
        "  n_columns = len(df.columns)\n",
        "  n_rows = n_columns\n",
        "  correlations = np.ones((n_rows, n_columns)) * np.nan\n",
        "  \n",
        "  for i in range(n_rows):\n",
        "    for j in range(n_columns):\n",
        "      try:\n",
        "        correlations[i, j] = mcf_corr(df.iloc[:, i].values, df.iloc[:, j].values)\n",
        "      except:\n",
        "        None\n",
        "\n",
        "  return correlations\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MKN0Z5JxRMO"
      },
      "source": [
        "### (c) Fixing the mystery function\n",
        "The `mystery` function is not that useful right now. Can you modify it, slightly, to make it more useful? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4kkwF1Vtsn9"
      },
      "source": [
        "## Exercise 4: Link functions\n",
        "We don't have to use the logistic or probit function, as `statsmodels` support many more. Consult the documentation for `statsmodels` and try out the Cauchy link function and the cloglog link function too, on the data set of\n",
        "\n",
        "> Indented block\n",
        "\n",
        "\n",
        "exercise 2. For instance, the probit link would look like:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyrMKOF8vFd8",
        "outputId": "9092434b-8b25-418a-c710-bfad1884f1bb"
      },
      "source": [
        "mod_probit = smf.glm(formula=\"Type_new ~ FirstLat\", data=hurricanes, family=sm.families.Binomial(sm.genmod.families.links.probit())).fit()\n",
        "print(mod_probit.summary())"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 Generalized Linear Model Regression Results                  \n",
            "==============================================================================\n",
            "Dep. Variable:               Type_new   No. Observations:                  337\n",
            "Model:                            GLM   Df Residuals:                      335\n",
            "Model Family:                Binomial   Df Model:                            1\n",
            "Link Function:                 probit   Scale:                          1.0000\n",
            "Method:                          IRLS   Log-Likelihood:                -116.72\n",
            "Date:                Thu, 16 Sep 2021   Deviance:                       233.44\n",
            "Time:                        08:34:02   Pearson chi2:                     754.\n",
            "No. Iterations:                     7                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "==============================================================================\n",
            "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "Intercept     -5.0284      0.471    -10.671      0.000      -5.952      -4.105\n",
            "FirstLat       0.2072      0.019     10.637      0.000       0.169       0.245\n",
            "==============================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPHdIj2rvia0"
      },
      "source": [
        "The documentation for the link functions is [here.](https://www.statsmodels.org/stable/glm.html#link-functions) It may be hard to get things to run, but persevere.\n",
        "\n",
        "Now plot the predicted values in the same plot, as we did in the lectures with probit and logit, with different colors for each. Is there i noticable difference in the curves? Is there a noticable difference in the McFadden $R^2$ values?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzzuTOaRxyCG"
      },
      "source": [
        "## Exerise 5: Efficency\n",
        "I have claimed that the maximum likelihood estimator / log score estimator is superior to the least squares estimator in terms of the asymptotic variance of the estimator. In this exercise, we explore what this means uing simulations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Oh0wfONyF1c"
      },
      "source": [
        "### (a) Simulation function\n",
        "Make a function `sim(a, b, x, rng)` that simulates a logistic model with parameters `a,b` and covariates `x`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAt2X7UJ7DUh"
      },
      "source": [
        "### (b) Implementing estimators\n",
        "Now we proceed to simulating estimators. An estimator is, in this context, a functon that `est(x,y)` that returns an `np.array` with the elements `a` and `b`, the parameters in $F(a + bx)$. (*Hint:* Remember the `values` attribute of a data frame object.) \n",
        "\n",
        "In addition to the `probit` estimator below, write a `logit` estimator and a \n",
        "logistic regression estimator using `curve_fit`, e.g. `logit_cf`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWaw43577_zo",
        "outputId": "54ea2b75-da91-410c-bba6-fae90441bb53"
      },
      "source": [
        "def probit(x, y):\n",
        "  fit = smf.glm(formula=\"y ~ x\", \n",
        "          data=pd.DataFrame({'x':x, 'y':y}), \n",
        "          family=sm.families.Binomial(sm.genmod.families.links.probit())).fit()\n",
        "  return fit.params.values\n",
        "\n",
        "x = np.linspace(-1, 1, 100)\n",
        "probit(x, sim(1, 2, x, rng)) "
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.9568136 , 2.16115886])"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwdcGrITzDJ9"
      },
      "source": [
        "### (c) Simulate estimates function\n",
        "Now we want to make a  `sim_estimates` function that simulates the estimators. Let `estimators` be a list of estimators. Complete function below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZZy2qEHGVu2"
      },
      "source": [
        "def sim_estimates(estimators, a, b, x, n_reps, rng):\n",
        "  \"\"\" Simulate n_reps logistic regression models (y). Each time, fit every funtion \n",
        "  in estimators to x and y. Return the parameter estimates in a suitable array.\"\"\"\n",
        "  n_estimators = len(estimators)\n",
        "  estimates = np.ones((n_reps, 2 * n_estimators))\n",
        "  for row in range(n_reps):\n",
        "    # fill in\n",
        "    for index, estimator in enumerate(estimators):\n",
        "      # fill in\n",
        "  return estimates\n",
        "\n",
        "n_reps = 1000\n",
        "sims = sim_estimates([logit_cf, logit], 1, 2, x, n_reps, rng)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tAAJHn70uVV"
      },
      "source": [
        "### (d) Simulate the variances and mean squared errors\n",
        "Use `n_reps = 10000` to simulate the variances nad mean squared error for `logit`, `probit`, and `logit_cf`. Which estimator is best?\n",
        "*Hint:* Remember `np.apply_along_axis`? Use that together with `np.var`. The mean squared error is defined as `np.mean((a_hat - a)**2)`, where a is the true parameter value and `a_hat` is the estimator. It is the most common measure of estimator quality, with smaller numbers being better.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfDVgikIDti9"
      },
      "source": [
        "### (e) Make histogram of estimates\n",
        "Use `n_reps = 10000` and make histograms of all the estimators in a nice grid, so you can compare them easily. "
      ]
    }
  ]
}