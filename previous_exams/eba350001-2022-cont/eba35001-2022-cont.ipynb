{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"EBA35001 Fall 2022\"\n",
        "subtitle: Take home exam\n",
        "author: Jonas Moss\n",
        "format:\n",
        "  pdf:\n",
        "    colorlinks: true\n",
        "editor: visual\n",
        "---"
      ],
      "id": "2c2669c7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction\n",
        "\n",
        "1.  Only show output that supports your argument. If you use Jupyter Notebooks, you may hide the output of a cell using a semi-colon `;`. We will deduct points from shoddily written reports plagued by noisy outputs.\n",
        "\n",
        "2.  Make your plots look nice. Add appropriate axis labels, legends and so on.\n",
        "\n",
        "3.  *\"Brevity is the soul of wit.\"* Strive not to write too much. We prefer pithy to lengthy expositions.\n",
        "\n",
        "4.  The exercises are equally weighted. Every subexercise is equally scored, with a minimum score of 0 and a maximum of 2. Since there are 30 subexercises in total, you can get a maximum of 60 points.\n"
      ],
      "id": "9a3f599c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels as sm\n",
        "import statsmodels.formula.api as smf"
      ],
      "id": "a684c112",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1 Binary regression\n",
        "\n",
        "We're using the [Wine quality](https://www.kaggle.com/datasets/rajyellow46/wine-quality) data set in this exercise. Take a look at the page for more information.\n",
        "\n",
        "## (a)\n",
        "\n",
        "### (i)\n",
        "\n",
        "Import the data set `winequalityN.csv` as `wine`. We want to use the data to deduce if a wine is acceptable, with acceptable defined as `quality > 5`. Define a column `acceptable` that contains `True`s when `quality > 5` and `False`s otherwise. We won't use `quality` anymore, so drop it from the table. Finally, drop all rows containing `NA`s.\n",
        "\n",
        "#### Solution\n"
      ],
      "id": "639b7edb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "wine = pd.read_csv(\"winequalityN.csv\")\n",
        "wine[\"acceptable\"] = wine.quality > 5\n",
        "wine = wine.drop(columns = \"quality\")\n",
        "wine = wine.dropna()"
      ],
      "id": "838af599",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (ii)\n",
        "\n",
        "Make a `pairplot` of the data. Take note of any patterns. (*Hint*: If `pairplot` is slow, you can use the `sample` method to reduce the data strain.)\n",
        "\n",
        "#### Solution\n"
      ],
      "id": "5fa5de22"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#sns.pairplot(wine.sample(1000), hue = \"acceptable\", kind = \"kde\")\n",
        "#plt.show()"
      ],
      "id": "f5ad3cb0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are no bivariate patterns discernible from the data, but at least one univariate pattern, namely that the acceptable wines tend to have lower alcohol content. They also have higher \"density\".\n",
        "\n",
        "### (iii)\n",
        "\n",
        "Display the correlation matrix between the numerical values in the data. Then find the columns where the difference between the correlations for the bad wines and good wines is greater than $0.15$, along with both correlations. (*Hint:* You may need to iterate over all the column names and use a `set`.)\n",
        "\n",
        "#### Solution\n"
      ],
      "id": "5f100f60"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#wine.corr(numeric_only = True)"
      ],
      "id": "eab14c2d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the second part, first define\n"
      ],
      "id": "931ef391"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "good = wine[wine.acceptable].corr(numeric_only = True)\n",
        "bad = wine[wine.acceptable == False].corr(numeric_only = True)"
      ],
      "id": "bf8ea2eb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then find the indices where the difference is greater than $0.15$.\n"
      ],
      "id": "79a51d24"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "indices = (good - bad).abs() > 0.15 \n",
        "\n",
        "names = wine.columns[range(1, 12)]"
      ],
      "id": "12765c21",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "combs = set()\n",
        "names = wine.columns[range(1, 12)]\n",
        "for name1 in names:\n",
        "  index = 0\n",
        "  for name2 in names:\n",
        "    if indices[name1][index]:\n",
        "      combs.add((name1, name2, good[name1][index], bad[name1][index]))\n",
        "    index += 1\n",
        "combs"
      ],
      "id": "38af7a18",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## (b)\n",
        "\n",
        "### (i)\n",
        "\n",
        "Run logistic regression model on all covariates with `acceptable` as response variable. Make a `significant` array containing all covariates that are significant at the $0.05$ level (along with their *p*-values) and a `not_significant` array containing the rest. Print those two arrays.\n",
        "\n",
        "#### Solution\n"
      ],
      "id": "14ae548c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = smf.logit(\"I(acceptable*1) ~ Q('fixed acidity') + Q('volatile acidity') + Q('citric acid') + Q('residual sugar') + chlorides + Q('free sulfur dioxide') + Q('total sulfur dioxide') + density + pH + sulphates + alcohol\", data = wine).fit()"
      ],
      "id": "d9b37e98",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pvalues = model.pvalues\n",
        "significant = pvalues[pvalues <= 0.05]\n",
        "not_significant = pvalues[pvalues > 0.05]\n",
        "significant\n",
        "not_significant"
      ],
      "id": "00582d58",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (ii)\n",
        "\n",
        "Fit a new regression model with the non-significant covariates removed, but keeping the intercept. Which model do you prefer?\n",
        "\n",
        "#### Solution\n",
        "\n",
        "We fit the model and look at the AIC.\n"
      ],
      "id": "1fc290cb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model2 = smf.logit(\"I(acceptable*1) ~ Q('volatile acidity') + Q('citric acid') + Q('residual sugar') + Q('free sulfur dioxide') + Q('total sulfur dioxide') + pH + sulphates + alcohol\", data = wine).fit()\n",
        "model.aic\n",
        "model2.aic"
      ],
      "id": "549eb19c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The AIC of the second model is lower, so I prefer it.\n",
        "\n",
        "### (iii)\n",
        "\n",
        "Using the same two models as in (i) and (ii), change the link function from the logistic link to the Probit link, Cauchit link, and cloglog link. Report the AICs of the models in a table like this:\n",
        "\n",
        "|         | Logistic | Probit | Cauchit | Cloglog |\n",
        "|---------|----------|--------|---------|---------|\n",
        "| Model 1 |          |        |         |         |\n",
        "| Model 2 |          |        |         |         |\n",
        "\n",
        "(*Hint*: You need to take a good look at the documentation of the `glm` function of statsmodels. Also see the lecture notes.)\n",
        "\n",
        "#### Solution\n",
        "\n",
        "Skipped.\n",
        "\n",
        "## (c)\n",
        "\n",
        "### (i)\n",
        "\n",
        "The alcohol covariate appears to have a stronger influence than the other covariates. Fit three models using: (a) `log(alcohol + 1)`, (b) `alcohol**2`, (c) `log(alcohol + 1) + alcohol^2`, and report their AICs. Is any of the new models performing better than the others?\n",
        "\n",
        "#### Solution\n"
      ],
      "id": "47e03c31"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model3 = smf.logit(\"I(acceptable*1) ~ Q('volatile acidity') + Q('citric acid') + Q('residual sugar') + Q('free sulfur dioxide') + Q('total sulfur dioxide') + pH + sulphates + alcohol + I(np.log(1 + alcohol))\", data = wine).fit()\n",
        "model4 = smf.logit(\"I(acceptable*1) ~ Q('volatile acidity') + Q('citric acid') + Q('residual sugar') + Q('free sulfur dioxide') + Q('total sulfur dioxide') + pH + sulphates + alcohol + I(alcohol**2)\", data = wine).fit()\n",
        "model5 = smf.logit(\"I(acceptable*1) ~ Q('volatile acidity') + Q('citric acid') + Q('residual sugar') + Q('free sulfur dioxide') + Q('total sulfur dioxide') + pH + sulphates + alcohol+ I(np.log(1 + alcohol)) + I(alcohol**2)\", data = wine).fit()\n",
        "model.aic\n",
        "model2.aic\n",
        "model3.aic\n",
        "model4.aic\n",
        "model5.aic"
      ],
      "id": "b06e52b9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that the model with both the log term and the quadratic term performs better.\n",
        "\n",
        "### (ii)\n",
        "\n",
        "Fit at least five additional models and report their results in a table containing the formula and the resulting AIC, plus potentially more information. Pick your favorite among these.\n",
        "\n",
        "#### Solution\n",
        "\n",
        "Skipped.\n",
        "\n",
        "### (iii)\n",
        "\n",
        "Make a receiver operating characterstic curve for your favorite model. Explain what it means.\n",
        "\n",
        "#### Solution\n"
      ],
      "id": "76a4f86c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "fpr, tpr, _ = metrics.roc_curve(wine.acceptable,  model5.predict(wine))\n",
        "plt.clf()\n",
        "plt.plot(fpr,tpr)\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.show()"
      ],
      "id": "65fcd584",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (iv)\n",
        "\n",
        "Calculate the AUC for the models you tested in exercise (iii).\n",
        "\n",
        "#### Solution\n",
        "\n",
        "Skipped.\n",
        "\n",
        "# 2 Linear regression\n",
        "\n",
        "We use the [student performance prediction](https://www.kaggle.com/datasets/rkiattisak/student-performance-in-mathematics), available in the `exams.csv` file. See the page for more information about this data set.\n",
        "\n",
        "## (a)\n",
        "\n",
        "### (i)\n",
        "\n",
        "Import the exams file into the variable `exams`. It is well-known that general intelligence encompasses both math skill and literary skill. Display the correlation matrix between math skill, literary skill, and writing skill.\n",
        "\n",
        "#### Solution\n"
      ],
      "id": "eba40ff6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "exams.corr()"
      ],
      "id": "0982abd9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (ii)\n",
        "\n",
        "Are the correlations in the previous exercise individually significantly different from $0$? You can use any valid method to figure this out, but you might want to use linear regression. Don't use the summary method to display the *p*-values, as it takes too much space. (*Hint:* Remember to use the `Q` function to access columns with spaces inside.)\n",
        "\n",
        "#### Solution\n",
        "\n",
        "Use linear regression as follows:\n"
      ],
      "id": "7c60c8cc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "smf.ols(\"Q('math score') ~ Q('writing score')\", data = exams).fit().pvalues[1]"
      ],
      "id": "dcac1cdd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "smf.ols(\"Q('math score') ~ Q('reading score')\", data = exams).fit().pvalues[1]"
      ],
      "id": "309f4324",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "smf.ols(\"Q('reading score') ~ Q('writing score')\", data = exams).fit().pvalues[1]"
      ],
      "id": "aa6f33e3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (iii)\n",
        "\n",
        "Find the optimal linear combination of `writing score` and `reading score` to predict `math score`. What is the correlation between `math score` and this optimal linear combination? Recall that a linear combination is on the form `a + b * writing score + c * reading score`.\n",
        "\n",
        "#### Solution\n",
        "\n",
        "Make a fit model.\n"
      ],
      "id": "2dd181f8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fit = smf.ols(\"Q('math score') ~ Q('writing score') + Q('reading score')\", data = exams).fit()"
      ],
      "id": "6259ea60",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The parameters of this optimal combination are\n"
      ],
      "id": "5e6884a2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fit.params"
      ],
      "id": "aa5f062d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And the correlation is the square root of the $R^2$, i.e.,\n"
      ],
      "id": "171694de"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "np.sqrt(fit.rsquared)"
      ],
      "id": "2edf1f6e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The correlation is positive since the coefficients of `writing score` and `reading score` as positive.\n",
        "\n",
        "## (b)\n",
        "\n",
        "### (i)\n",
        "\n",
        "Display the distinct categories in every column that contains only categorical values.\n",
        "\n",
        "#### Solution\n"
      ],
      "id": "992ac8e4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "exams.gender.unique()\n",
        "exams[\"race/ethnicity\"].unique()\n",
        "exams[\"parental level of education\"].unique()\n",
        "exams[\"test preparation course\"].unique()"
      ],
      "id": "8ae64e5d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (ii)\n",
        "\n",
        "Fit a regression model on `math score` using all covariates except `writing skill` and `reading skill`. Show its summary table. Should you report the adjusted $R^2$ or the ordinary $R^2$ for this model?\n",
        "\n",
        "#### Solution\n",
        "\n",
        "You should report the adjusted $R^2$, as the model contains many covariates.\n"
      ],
      "id": "713fbad0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "smf.ols(\"Q('math score') ~ gender + Q('race/ethnicity') + Q('parental level of education') + Q('test preparation course')\", data = exams).fit().summary()"
      ],
      "id": "7f698fa9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (iii)\n",
        "\n",
        "From the output above it looks like the results are roughly linear in level of education of the parents. Add a new column (called `education numeric`) to the data where the level of education is numeric, i.e., `some high school` is mapped to $1$, `high school` to $2$, and so on. (*Hint*: Google `pandas replace`.) Run a linear regression using `education numeric` instead of `parental level of education`. Would you prefer to use this model or the last model?\n",
        "\n",
        "#### Solution\n",
        "\n",
        "Use `replace` to make a new column.\n"
      ],
      "id": "0d2bd806"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "exams['education numeric'] = exams['parental level of education'].replace(['some high school', 'high school', 'some college', \"associate's degree\",\"bachelor's degree\", \"master's degree\"],[1, 2, 3, 4, 5, 6])"
      ],
      "id": "43379b9c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then run a linear regression.\n"
      ],
      "id": "0013a12f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "smf.ols(\"Q('math score') ~ gender + Q('race/ethnicity') + Q('education numeric') + Q('test preparation course')\", data = exams).fit().summary()"
      ],
      "id": "66a93258",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I would prefer to use this model, as the adjusted $R^2$s are equal, and this one is simpler. But any reasonable answer is OK here.\n",
        "\n",
        "## (c)\n",
        "\n",
        "### (i)\n",
        "\n",
        "Run the model `Q('writing score') ~ gender + Q('race/ethnicity')` and display its parameter estimates. What is the interpretation of `gender[T.male]` and `Q('race/ethnicity')[T.group E]`?\n",
        "\n",
        "#### Solution\n"
      ],
      "id": "c9ac0ca6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fit = smf.ols(\"Q('writing score') ~ gender + Q('race/ethnicity')\", data = exams).fit()"
      ],
      "id": "740be2d6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (ii)\n",
        "\n",
        "Referring to the model in (i), we would like to check if it is possible to do a similar trick as we did for level of education, linearizing the categories. Plot the estimated values for `Q('race/ethnicity')` (in the appropriate order) against `[0, 1, 2, 3, 4]` and see if there is a pattern.\n",
        "\n",
        "#### Solution\n"
      ],
      "id": "a4b6eff8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "x = [0] + fit.params[[2, 3, 4, 5]].tolist()\n",
        "plt.clf()\n",
        "plt.plot([0, 1, 2, 3, 4], x)\n",
        "plt.show()"
      ],
      "id": "cb25e46c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (iii)\n",
        "\n",
        "Fit a suitable function to the data obtained in the previous exercise. Make the plot of the function and evaluate its fit to the data.\n",
        "\n",
        "#### Solution\n",
        "\n",
        "We use a quadratic function.\n"
      ],
      "id": "be285d4c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dat = pd.DataFrame({\"y\": x, \"x\": [0, 1, 2, 3, 4]})\n",
        "mod = smf.ols(\"y ~ x + I(x**2)\", data = dat).fit()"
      ],
      "id": "2a9848f2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can plot it as follows:\n"
      ],
      "id": "7ffbe3bd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "z = np.linspace(0, 4, 100)\n",
        "x = [0] + fit.params[[2, 3, 4, 5]].tolist()\n",
        "plt.clf()\n",
        "plt.plot([0, 1, 2, 3, 4], x)\n",
        "plt.plot(z, mod.predict(pd.DataFrame({\"x\": z})))\n",
        "plt.show()"
      ],
      "id": "79876a75",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This looks like a very decent fit.\n",
        "\n",
        "### (iv)\n",
        "\n",
        "Replace the categorical values of `race/ethnicity` with the predicted values in a column `race numeric` (use the values found in the previous exercise).Run the regression in (i) again, but with `race numeric` instead of `race/ethnicity`. Which model do you prefer?\n",
        "\n",
        "#### Solution\n",
        "\n",
        "And replace the values\n"
      ],
      "id": "1bee097b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "exams[\"race numeric\"] = exams[\"race/ethnicity\"].replace([\"group A\", \"group B\", \"group C\", \"group D\", \"group E\"],[0,1,2,3,4])\n",
        "exams"
      ],
      "id": "2b2c98b9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can run the regressions:\n"
      ],
      "id": "2076d37d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fit1 = smf.ols(\"Q('writing score') ~ gender + Q('race/ethnicity')\", data = exams).fit()\n",
        "fit2 = smf.ols(\"Q('writing score') ~ gender + Q('race numeric')\", data = exams).fit()\n",
        "fit1.aic\n",
        "fit2.aic"
      ],
      "id": "ab75581d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The AIC is much lower for the first model, so I prefer that one. \\# 3 Simulations\n",
        "\n",
        "# 3 Simulations\n",
        "\n",
        "## (a)\n",
        "\n",
        "The central limit theorem states that $\\sqrt{n}(\\overline{X}_n - \\mu) / \\sigma \\to N(0,1)$ when $X_1,X_2,\\ldots,X_n$ are iid with mean $\\mu$ and standard deviation $\\sigma$, and the empirical mean is $\\overline{X}_n = (\\sum_{i=1}^n X_i) / n$ and $N(0,1)$ is the standard normal. It's often interesting to see how quick the convergence is; we'll explore that in this problem.\n",
        "\n",
        "### (i)\n",
        "\n",
        "Make a function `simmean(n, model, n_reps = 10000)`. It simulates `n_reps` times $n$ observations from the random number generator `model` and calculates its empirical mean and empirical standard deviation. Make a histogram (with densities, not counts) of the simulations when the model is the function `np.random.default_rng(seed = 313).exponential(1, dim)`\n",
        "\n",
        "#### Solution\n"
      ],
      "id": "19fb29ee"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def simmean(n, model, n_reps = 10000):\n",
        "  x = model((n, n_reps))\n",
        "  return x.mean(axis = 0)\n",
        "\n",
        "plt.clf()\n",
        "plt.hist(simmean(100, lambda dim: np.random.default_rng(seed = 313).exponential(1, dim)), density = True)\n",
        "plt.show()"
      ],
      "id": "025171f2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (ii)\n",
        "\n",
        "Make a function `simclt` that extends to function above with `mu` and `sigma`, and returns samples from $\\sqrt{n}(\\overline{X}_n - \\mu) / \\sigma$. Make a (density) histogram for the same input as above, and overlay a standard normal on top of the plot.\n",
        "\n",
        "#### Solution\n"
      ],
      "id": "790f05bb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from scipy.stats import norm\n",
        "def simclt(n, model, mu, sigma, n_reps = 10000):\n",
        "  x = model((n, n_reps))\n",
        "  return (x.mean(axis = 0) - mu) * np.sqrt(n) / sigma\n",
        "\n",
        "plt.clf()\n",
        "plt.hist(simclt(100, lambda dim: np.random.default_rng(seed = 313).exponential(1, dim), mu = 1, sigma = 1), density = True)\n",
        "x = np.linspace(-4, 4, 100)\n",
        "plt.plot(x, norm.pdf(x, 0, 1))\n",
        "plt.show()"
      ],
      "id": "08d61f5b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (iii)\n",
        "\n",
        "Make similar plots for the Pareto distribution with parameter $b=3$ (following the Scipy convention) for $n=10, n=100, n=1000$, and extend the exponential analysis to $n=10$ and $n=1000$. Comment on the results. You need to figure out the mean and standard deviation for the Pareto yourself. (*Hint:* See the Scipy documentation and wikipedia. Observe that `Numpy` shifts the Pareto distribution towards `0`; see the Numpy docs for details.)\n",
        "\n",
        "#### Solution\n",
        "\n",
        "We see that the CLT kick in slower for Pareto than the exponential.\n"
      ],
      "id": "81af4811"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.clf()\n",
        "plt.hist(simclt(10, lambda dim: np.random.default_rng(seed = 313).exponential(1, dim), mu = 1, sigma = 1), density = True)\n",
        "plt.hist(simclt(100, lambda dim: np.random.default_rng(seed = 313).exponential(1, dim), mu = 1, sigma = 1), density = True)\n",
        "plt.hist(simclt(1000, lambda dim: np.random.default_rng(seed = 313).exponential(1, dim), mu = 1, sigma = 1), density = True)\n",
        "x = np.linspace(-4, 4, 100)\n",
        "plt.plot(x, norm.pdf(x, 0, 1))\n",
        "plt.show()\n",
        "\n",
        "from scipy.stats import pareto\n",
        "b = 3\n",
        "mu = b / (b - 1) - 1\n",
        "sigma = np.sqrt(b / ((b - 1)**2 * (b - 2)))\n",
        "\n",
        "\n",
        "plt.clf()\n",
        "rng = np.random.default_rng(seed = 313)\n",
        "plt.hist(simclt(10, lambda dim: rng.pareto(b, dim), mu = mu, sigma = sigma), density = True)\n",
        "plt.hist(simclt(100, lambda dim: rng.pareto(b, dim), mu = mu, sigma = sigma), density = True)\n",
        "plt.hist(simclt(1000, lambda dim: rng.pareto(b, dim), mu = mu, sigma = sigma), density = True)\n",
        "x = np.linspace(-4, 4, 100)\n",
        "plt.plot(x, norm.pdf(x, 0, 1))\n",
        "plt.show()"
      ],
      "id": "78cfac3a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## (b)\n",
        "\n",
        "### (i)\n",
        "\n",
        "Make a function `simmax` that simulates $n$ observations from a standard exponential distribution and finds the maximum of the observations. It must take an `rng` as input. Use it to simulate the maximum of $n = 1000$ exponentials when `rng = np.random.default_rng(seed = 313)`.\n",
        "\n",
        "#### Solution\n"
      ],
      "id": "6e9473f7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "rng = np.random.default_rng(seed = 313)\n",
        "def simmax(n, rng):\n",
        "  return rng.exponential(1, 1000).max()\n",
        "\n",
        "simmax(1000, rng)"
      ],
      "id": "493d2b12",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (ii)\n",
        "\n",
        "Extend the function above to `simmaxs`, a function that take an `n_reps = 10,000` argument in addition to `n` and `rng`. It should return a Numpy array with `n_reps` independent simulations of the maximum. Make a histogram of \"maxima-$\\log(n)$\", where $n = 100$ and `n_reps = 10,000`. Make sure the histogram displays the density of the maxima, not the frequency count.\n",
        "\n",
        "#### Solution\n"
      ],
      "id": "5016b892"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "rng = np.random.default_rng(seed = 313)\n",
        "def simmaxs(n, rng, n_reps = 10000):\n",
        "  return rng.exponential(1, (n_reps, n)).max(axis = 1)\n",
        "\n",
        "plt.clf()\n",
        "plt.hist(simmaxs(100, rng) - np.log(100), density = True)\n",
        "plt.show()"
      ],
      "id": "fd003687",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (iii)\n",
        "\n",
        "Plot the standard Gumbel distribution on top of the histogram. It is part of Scipy, called `gumbel_r`. Make similar plots for $n=1000$ and $n=10000$. What do you see?\n",
        "\n",
        "#### Solution\n"
      ],
      "id": "9688896f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import scipy.stats as stats\n",
        "x = np.linspace(-10, 20, 100)\n",
        "plt.plot(x, stats.gumbel_r.pdf(x))\n",
        "plt.hist(simmaxs(1000, rng) - np.log(1000), density = True )\n",
        "plt.hist(simmaxs(10000, rng) - np.log(10000), density = True )\n",
        "plt.show()"
      ],
      "id": "d848797a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It appears that the maxima converges to a standard Gumbel as $n\\to\\infty$.\n",
        "\n",
        "## (c)\n",
        "\n",
        "### (i)\n",
        "\n",
        "For a total of `n_reps = 10,000` times, draw $n=100$ samples from the standard Cauchy distribution (`standard_t` with degrees of freedom equal to $1$) and calculate the mean over these $100$ values. Then make a histogram of its mean. Make sure the histogram shows a density, not the frequency of counts, give it `100` bins, and restrict it to the range `(-50, 50)`.\n",
        "\n",
        "#### Solution\n"
      ],
      "id": "3c271147"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "n = 100\n",
        "n_reps = 10000\n",
        "rng = np.random.default_rng(seed = 313)\n",
        "x = rng.standard_t(1, (n_reps, n))\n",
        "sims = x.mean(axis = 1)\n",
        "plt.clf()\n",
        "plt.hist(sims, range = (-50, 50), bins = 100, density = True)\n",
        "plt.show()\n",
        "sims"
      ],
      "id": "f21697a9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (ii)\n",
        "\n",
        "Generalize the previous exercise by making a function taking `n` and `n_reps = 10000` as arguments, returning the simulated values. Make three histograms for `n=10`, `n=100` and `n=10,000`. What do you see?\n",
        "\n",
        "#### Solution\n"
      ],
      "id": "cfac030c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "rng = np.random.default_rng(seed = 313)\n",
        "def simfun(n, rng, n_reps = 10000):\n",
        "  x = rng.standard_t(1, (n_reps, n))\n",
        "  return x.mean(axis = 1)\n",
        "\n",
        "plt.clf()\n",
        "plt.hist(simfun(10, rng), range = (-50, 50), bins = 100, density = True)\n",
        "plt.hist(simfun(100, rng), range = (-50, 50), bins = 100, density = True)\n",
        "plt.hist(simfun(1000, rng), range = (-50, 50), bins = 100, density = True)\n",
        "plt.show()\n",
        "sims"
      ],
      "id": "cc34759e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All histograms are roughly equal!\n",
        "\n",
        "### (iii)\n",
        "\n",
        "Add a curve for the standard Cauchy distribution to the histograms of the last exercise. What do you see? (*Hint:* Use `Scipy`.)\n",
        "\n",
        "#### Solution\n"
      ],
      "id": "30a8e5ab"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from scipy.stats import cauchy\n",
        "x = np.linspace(-50, 50, 1000)\n",
        "plt.plot(x, cauchy.pdf(x))\n",
        "plt.show()"
      ],
      "id": "4c71e6bd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (iv)\n",
        "\n",
        "Does the central limit theorem hold for a sequence of iid Cauchy random variables? Why or why not? Demonstrate using a suitable simulation.\n",
        "\n",
        "#### Solution\n",
        "\n",
        "No, it does not hold. You can see that by observing the previous simulation \"stabilized\" when dividing by `n`. If you divide by $\\sqrt{n}$, the histogram will explode, taking on larger and larger values as $n$ increases."
      ],
      "id": "abc9f14b"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}