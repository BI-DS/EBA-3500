{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cXdeN6tD7Z7S"
   },
   "source": [
    "# EBA3500 Lecture 11. Expectation, consistency, and the adjusted $R^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of expectation and variance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expectation\n",
    "Recall the expectation $E(X)$, which equals the theoretical mean of a random variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition (Expectation)\n",
    "> Let $X$ be a discrete random variable with probability mass function $p(x) = P(X=x)$. Then $E(X) = \\sum xp(x)$ is the *expectation* or *expected value* of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** If $X$ isn't discrete, which is usually the case, we would have to use the integral instead of the sum, i.e. $E(X) = \\int xp(x)dx$. There is *no difference* in interpretation though. The expectation is "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example (Bernoulli)\n",
    "Let $X$ be Bernoulli distributed with success probability $\\pi$, i.e., $p(X=1) = \\pi$ and $p(X=0) = 1 - \\pi$. Then \n",
    "$$p(X=1)\\cdot1 + p(X=0)\\cdot 0 = \\pi\\cdot1 + (1-\\pi) \\cdot 0 = 0,$$\n",
    "hence $E(X) = \\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expectation operator has a  momentous property, namely that of being linear. This property allows us to calculate stuff without using the definition of the expectation given above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proposition: Linearity of expectation\n",
    "> Let $X_1, X_2$ be random variables and $a, b$ be numbers. Then\n",
    "$E(aX_1 + bX_2) = aEX_1+bEX_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, function is linear if it acts like this, for instance $f(ax+by) = af(x) + bf(y)$ where $a,b$ are numbers and $x,y$ are some kind of mathematical objects. For instance, matrix multiplication is linear, $A(ax+by) = aA(x) + bA(y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example (Bernoulli)\n",
    "Suppose $X_1$ and $X_2$ are Bernoulli variables with parameters $\\pi_1$ and $\\pi_2$. What is the expectation of $X_1 + X_2$? \n",
    "Using the linearity of expectation, we find that $\\pi_1 + \\pi_2$! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance\n",
    "The variance of a random variable $X$ captures its dispersion, or how spread out it is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition (Variance)\n",
    "> Let $X$ be a discrete random variable. Then $$\\textrm{Var}(X) = E(X^2) - E(X)^2$$ is the *variance* or of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example (Bernoulli)\n",
    "Let $X$ be Bernoulli distributed with success probability $\\pi$, i.e., $p(X=1) = \\pi$ and $p(X=0) = 1 - \\pi$. Then $E(X) = \\pi$, so we can calculate the $E(X)^2 = \\pi^2$ part of the variance equation. To calculate the $EX^2$ part, use\n",
    "$$p(X=1)\\cdot1^2 + p(X=0)\\cdot 0^2 = \\pi\\cdot1^2+ (1-\\pi) \\cdot 0 = 0^2.$$\n",
    "Hence $\\textrm{Var}(X) = \\pi - \\pi^2 = \\pi(1-\\pi)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimator\n",
    "Let $\\theta$ be some population value, e.g., an expectation $E(X)$ of some sort, or a regression coefficient. This value is typically unknown. An estimator $\\hat{\\theta}_n$ is a statistical measurement, based on observed data, of this population value. Whenever we say a population value, think about input to the data-generating process: These are the exact values the creator of a simulation study decides on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition (Estimator) ([Wikipedia source](https://en.wikipedia.org/wiki/Estimator))\n",
    "> In statistics, an estimator is a rule for calculating an estimate of a given quantity based on observed data: thus the rule (the estimator), the quantity of interest (the estimand) and its result (the estimate) are distinguished. For example, the sample mean is a commonly used estimator of the population mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples\n",
    "1. Let $Y \\sim \\beta_0 + \\beta_1 X + \\epsilon$ and $\\hat{\\beta_1}$ be the regression coefficient estimated by least squares or least absolute deviations. Then $\\hat{\\beta_1}$ is an estimator of $\\beta_1$, and $\\beta_1$ is the estimand of $\\hat{\\beta_1}$.\n",
    "2. Let $x = x_1, x_2, ..., x_n$ be some observed data. Then the mean (i.e., `np.mean(x)`) of $x$ is an estimator of the theoretical population mean $\\mu$. \n",
    "3. Suppose we have a regression model with $p$ covariates and calculate its $R^2$. Then $R^2$ is an estimator of the true population $R^2$. \n",
    "4. Let, again, $x = x_1, x_2, ..., x_n$ be some observed data. Then the median (i.e., `np.mean(x)`) of $x$ is an estimator of the theoretical population median. \n",
    "5. Is a *p*-value an estimator? No, as it does not attempt to measure a population value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theoretical quantities\n",
    "The theoretical quantities, such as the population mean, population median, and population $R^2$, are quite hard to understand. Many people never understand them at all, but you are different! ðŸ˜Š They are properties of the data-generating process. Recall the intuition behind the data generating process. Alice, or Vishnu, or any other entity, has a program that generates your observed data. If you *knew* that program, you would have been able to calculate anything you'd ever want to. But you don't, and that's why you need to estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.4591469 , 0.767279  , 0.24938788, 0.90998112, 0.49108449,\n",
       "       1.12724964, 2.24739775, 0.37583599, 4.02495832, 0.49735181])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## This is Alice who simulates for Bob. Bob never sees the program.\n",
    "import numpy as np\n",
    "rng = np.random.default_rng(seed = 313)\n",
    "u = rng.random(10) # uniformly distributed variables on [0,1]\n",
    "x = -np.log(u)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random variable $U$ is uniform, and has theoretical mean (expectation) $1/2$. Bob doesn't know what Alice is doing though, so he has to estimate the mean. The random variable $X$ also have an expectation, namely $1$. (If you know integral calculus, you can calculate this yourself!) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition (Unbiased estimator)\n",
    "> An estimator is *unbiased* if $E(\\hat{\\theta}_n) = \\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most popular estimators are not unbiased, and it is *not* an important property in most scenarios. For instance, the estimated regression coefficients in a logistic regression are not unbiased. Neither are the estimated regression coefficients when using least absolute deviations. However, the sampled variance $S^2 = \\frac{1}{n-1}\\sum (X_i - \\overline{X})^2$ is unbiased. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition (Convergence in probability)\n",
    "> An estimator $\\hat{\\theta}_{n}$ converges in probability to $\\theta$ if $P(|\\hat{\\theta}_{n}-\\theta|>\\epsilon)\\to0$ for all $\\epsilon>0$ as $n\\to\\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition (Consistency)\n",
    "> An estimator $\\hat{\\theta}_n$ is *consistent* for $\\theta$ if it converges in probability to $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consistency roughly means that the histogram of an estimator will concentrate aribtrarily well around the true value when $n\\to\\infty$.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simulate from the normal distribution\n",
    "\n",
    "## Simulate the median\n",
    "## Simulate the mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the median and mean are consistent for the $\\mu$ parameter in the normal distribution. This is, in fact, true. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We conclude, informally, that the sample median isn't consistent for the mean of the exponential distribution. Do you understand why? \n",
    "\n",
    "This isn't a course in mathematics, and proving consistency is often quite difficult. It is important, however, to know what it means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposition\n",
    "> Suppose the model conditions for the linear regression model holds true. Then the regression coefficients $\\beta_i$ are unbiased, have variance converging to $0$, and are consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That an estimator $\\hat{\\theta}$ is unbiased and has variance converging to $0$ actually implies that $\\hat{\\theta}$ is consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposition\n",
    "> Suppose that $\\hat{\\theta}_n$ be unbiased for $\\theta$, i.e., $E(\\hat{\\theta}_n) = \\theta$. Moreover, suppose that the variance of $\\hat{\\theta}$ converges to $0$ as $n\\to\\infty$. Then $\\hat{\\theta}_n \\to \\theta$ in probability. In other words, $\\hat{\\theta}_n$ is consistent for $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Proof\n",
    "Let $\\sigma_{n}^{2}=\\textrm{Var}\\hat{\\theta}_{n}.$ By [Chebyshev's inequality](https://en.wikipedia.org/wiki/Chebyshev%27s_inequality),\n",
    "$$\n",
    "P(|\\hat{\\theta}_{n}-\\theta|\\geq\\epsilon)\\leq\\frac{\\sigma_{n}^{2}}{\\epsilon^{2}}.\n",
    "$$\n",
    "Let $\\epsilon$ be fixed. Since $\\sigma_{n}^{2}\\to0$ by assumption,\n",
    "$\\frac{\\sigma_{n}^{2}}{\\epsilon^{2}}\\to0$ as well. Then, since $P(|\\hat{\\theta}_{n}-\\theta|\\geq\\epsilon \\leq\\frac{\\sigma_{n}^{2}}{\\epsilon^{2}}$,\n",
    "we find that $P(|\\hat{\\theta}_{n}-\\theta|\\geq\\epsilon)$ too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corollary: Law of large numbers\n",
    "Assume $X_n$ is a sequence of identically distributed variables with common mean $\\mu$ and finite variance $\\sigma^2$. Let $\\overline{X}_n$ denote the mean, $\\overline{X}_n = n^{-1}\\sum_{i=1}^n{X_i}$. Then $X_n\\to\\mu$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proof\n",
    "Exercise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjusted $R^2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Too big $R^2$ values\n",
    "The $R^2$ is good for evaluating how well be can predict the outcome given our covariates, but it's not good for choosing between models. In a nutshell, it's not good for choosing between models since it doesn't correct for the bias that occurs when using the same data both to estimate the model parameters and evaluating model fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Constructing the adjusted $R^2$\n",
    "$$R^2 = 1 - \\frac{\\textrm{Sum of squares with predictors}}{\\textrm{Sum of squares without predictors}}$$\n",
    "\n",
    "We can show that $\\textrm{Sum of squares with predictors}$ is biased for its population value, the true sum of squares with predictors at our estimated regression coefficient. But we can correct for this bias! One can show that $$\\frac{n}{n-p-1}E(\\textrm{Sum of squares with predictors})$$ equals the true population sum of squares with predictors. Here $p$ is the number of estimated regression coefficinets, minus the intercept. Moreover, we can show that \n",
    "$$\\frac{n}{n-1} E(\\textrm{Sum of squares with predictors})$$ \n",
    "equals the true, population value of the sum of squares without predictors.\n",
    "\n",
    "It follows that a reasonable corrected $R^2$ is\n",
    "$$R_a^2 = 1 - \\frac{\\frac{n}{n-p-1} \\textrm{Sum of squares with predictors}}{\\frac{n}{n-1}\\textrm{Sum of squares without predictors}}$$\n",
    "\n",
    "Rearranging this, we find that\n",
    "$$ R_a ^ 2 = 1 - (1 - R^2) \\frac{n-1}{n-p-1} .$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** (i) The adjusted $R^2$, or $R^2_a$, can be less than $0$. Try to understand why. (ii) We haven't proved that the adjusted $R^2$ squared is unbiased. Do you think it is? Can you devise a simulation study to explore this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "1. The expectation of a random variable $X$ is denoted by $E(X)$, and equals the theorotical mean of random variable $X$.\n",
    "2. An estimator approximates a population value based on observed data.\n",
    "3. An estimator is *consistent* if it approximates the population value arbitrarily well as $n\\to \\infty$.\n",
    "4. An estimator $\\hat{\\theta}_n$ is *unbiased* if it equals $\\theta$ in expectation, i.e., $E[\\hat{\\theta}_n]=\\theta$.\n",
    "5. Unbiased estimatation is not important, but it makes sense to correct the $R^2$ for bias.\n",
    "6. One attempt at bias-corrected $R^2$ is the adjusted $R^2$."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8214f72d23884c802e2dc6bec8e87b93e644d7518e94910628cfbca0d96cb336"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit (windows store)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
